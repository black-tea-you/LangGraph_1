"""
ë…¸ë“œ 4: Eval Turn Guard (ì œì¶œ ì‹œ)
ì œì¶œ ì‹œ ëª¨ë“  í„´ì˜ í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , ëˆ„ë½ëœ í„´ì„ ì¬í‰ê°€
"""
import logging
import asyncio
from typing import Dict, Any, Optional
from datetime import datetime

from app.domain.langgraph.states import MainGraphState
from app.infrastructure.cache.redis_client import redis_client

logger = logging.getLogger(__name__)


async def eval_turn_submit_guard(state: MainGraphState) -> Dict[str, Any]:
    """
    ì œì¶œ ì‹œ 4ë²ˆ ê°€ë“œ ë…¸ë“œ
    
    ì—­í• :
    1. Stateì˜ messagesì—ì„œ ëª¨ë“  í„´ ì¶”ì¶œ (1 ~ current_turn-1)
    2. ê° í„´ì— ëŒ€í•´ Eval Turn SubGraphë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
    3. ëª¨ë“  í„´ í‰ê°€ ì™„ë£Œ í›„ turn_scores ë°˜í™˜
    4. ë‹¤ìŒ ë…¸ë“œ(í‰ê°€ í”Œë¡œìš°)ë¡œ ì§„í–‰
    
    âš ï¸ ì¤‘ìš”: ì¼ë°˜ ì±„íŒ…ì—ì„œëŠ” í‰ê°€ë¥¼ í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì œì¶œ ì‹œ ëª¨ë“  í„´ì„ ì²˜ìŒë¶€í„° í‰ê°€í•©ë‹ˆë‹¤.
    """
    session_id = state.get("session_id", "unknown")
    current_turn = state.get("current_turn", 0)
    
    logger.info("")
    logger.info("=" * 80)
    logger.info(f"[4. Eval Turn Guard] ì§„ì… - session_id: {session_id}, í˜„ì¬ í„´: {current_turn}")
    logger.info("=" * 80)
    logger.info("")
    
    try:
        # â˜… Submit ì‹œ Stateì˜ messagesì—ì„œ ëª¨ë“  í„´ì„ ì¶”ì¶œí•˜ì—¬ í™•ì‹¤í•˜ê²Œ í‰ê°€
        # ì¼ë°˜ ì±„íŒ…ì—ì„œëŠ” í‰ê°€ë¥¼ í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì œì¶œ ì‹œ ëª¨ë“  í„´ì„ ì²˜ìŒë¶€í„° í‰ê°€
        logger.info(f"[4. Eval Turn Guard] State ê¸°ë°˜ ëª¨ë“  í„´ í‰ê°€ ì‹œì‘ - session_id: {session_id}, current_turn: {current_turn}")
        
        # Stateì˜ messagesì—ì„œ ëª¨ë“  í„´ ì¶”ì¶œ
        messages = state.get("messages", [])
        logger.info(f"[4. Eval Turn Guard] ì „ì²´ messages ê°œìˆ˜: {len(messages)}")
        
        # ë””ë²„ê¹…: ë©”ì‹œì§€ êµ¬ì¡° í™•ì¸
        for idx, msg in enumerate(messages):
            if isinstance(msg, dict):
                logger.debug(f"[4. Eval Turn Guard] ë©”ì‹œì§€ {idx} (dict): turn={msg.get('turn')}, role={msg.get('role')}, type={msg.get('type')}, content_len={len(str(msg.get('content', '')))}")
            else:
                msg_turn = getattr(msg, "turn", None)
                msg_role = getattr(msg, "role", None)
                msg_type = getattr(msg, "type", None) if hasattr(msg, "type") else None
                msg_content = getattr(msg, "content", None)
                logger.debug(f"[4. Eval Turn Guard] ë©”ì‹œì§€ {idx} (object): turn={msg_turn}, role={msg_role}, type={msg_type}, content_len={len(str(msg_content)) if msg_content else 0}")
        
        # ì œì¶œ í„´(current_turn)ì€ í‰ê°€í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, 1 ~ (current_turn - 1)ë§Œ í‰ê°€
        turns_to_evaluate = list(range(1, current_turn))
        logger.info("")
        logger.info(f"[4. Eval Turn Guard] â­ í‰ê°€ ëŒ€ìƒ í„´: {turns_to_evaluate}")
        logger.info(f"[4. Eval Turn Guard] â­ ì´ {len(turns_to_evaluate)}ê°œ í„´ í‰ê°€ ì˜ˆì •")
        logger.info("")
        
        if not turns_to_evaluate:
            logger.info(f"[4. Eval Turn Guard] í‰ê°€í•  í„´ì´ ì—†ìŒ (ì²« ì œì¶œ)")
            logger.info("")
            return {
                "turn_scores": {},
                "updated_at": datetime.utcnow().isoformat(),
            }
        
        # ëª¨ë“  í„´ í‰ê°€
        # Stateì˜ messagesì—ì„œ turn ì •ë³´ë¡œ ì§ì ‘ ë©”ì‹œì§€ ì°¾ê¸° (Redis turn_mapping ë¶ˆí•„ìš”)
        logger.info("-" * 80)
        for idx, turn in enumerate(turns_to_evaluate, 1):
            logger.info("")
            logger.info(f"[4. Eval Turn Guard] [{idx}/{len(turns_to_evaluate)}] í„´ {turn} í‰ê°€ ì‹œì‘...")
            logger.info("")
            
            human_msg = None
            ai_msg = None
            
            # Stateì˜ messagesì—ì„œ turn ì •ë³´ë¡œ ì§ì ‘ ê²€ìƒ‰
            # dict í˜•íƒœ ë˜ëŠ” LangChain BaseMessage ê°ì²´ ëª¨ë‘ ì§€ì›
            for msg in messages:
                # turn ì •ë³´ ì¶”ì¶œ
                msg_turn = None
                msg_role = None
                msg_content = None
                
                if isinstance(msg, dict):
                    msg_turn = msg.get("turn")
                    msg_role = msg.get("role") or msg.get("type")  # role ìš°ì„ , ì—†ìœ¼ë©´ type
                    msg_content = msg.get("content")
                else:
                    # LangChain BaseMessage ê°ì²´
                    msg_turn = getattr(msg, "turn", None)
                    # role ì†ì„± ì‚¬ìš© (writer.pyì—ì„œ role ì†ì„±ì„ ì¶”ê°€í•¨)
                    msg_role = getattr(msg, "role", None)
                    # content ì¶”ì¶œ
                    if hasattr(msg, "content"):
                        msg_content = msg.content
                    else:
                        msg_content = str(msg)
                
                # ë””ë²„ê¹…: ë©”ì‹œì§€ ì •ë³´ ë¡œê¹…
                logger.debug(f"[4. Eval Turn Guard] ë©”ì‹œì§€ í™•ì¸ - turn: {msg_turn}, role: {msg_role}, content_len: {len(msg_content) if msg_content else 0}")
                
                # í•´ë‹¹ í„´ì˜ ë©”ì‹œì§€ì¸ì§€ í™•ì¸
                # turnì´ Noneì´ë©´ ë©”ì‹œì§€ ìˆœì„œë¡œ ì¶”ë¡  (ì¸ë±ìŠ¤ 0,1 = turn 1, ì¸ë±ìŠ¤ 2,3 = turn 2, ...)
                msg_idx = messages.index(msg) if msg in messages else -1
                inferred_turn = (msg_idx // 2) + 1 if msg_idx >= 0 else None
                
                # turnì´ ì¼ì¹˜í•˜ê±°ë‚˜, turnì´ Noneì´ê³  ì¶”ë¡ ëœ turnì´ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
                if msg_turn == turn or (msg_turn is None and inferred_turn == turn):
                    # role ë§¤í•‘: "user"/"human" -> human, "assistant"/"ai" -> ai
                    if msg_role in ["user", "human"]:
                        human_msg = msg_content
                        logger.debug(f"[4. Eval Turn Guard] í„´ {turn} Human ë©”ì‹œì§€ ë°œê²¬ (turn: {msg_turn}, ì¶”ë¡ : {inferred_turn})")
                    elif msg_role in ["assistant", "ai"]:
                        ai_msg = msg_content
                        logger.debug(f"[4. Eval Turn Guard] í„´ {turn} AI ë©”ì‹œì§€ ë°œê²¬ (turn: {msg_turn}, ì¶”ë¡ : {inferred_turn})")
                    
                    # ë‘˜ ë‹¤ ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨
                    if human_msg and ai_msg:
                        break
            
            if human_msg and ai_msg:
                logger.info(f"[4. Eval Turn Guard] í„´ {turn} ë©”ì‹œì§€ ì¶”ì¶œ ì„±ê³µ - Stateì—ì„œ ì§ì ‘ ì¡°íšŒ")
            else:
                logger.warning(f"[4. Eval Turn Guard] í„´ {turn} - Stateì—ì„œ ë©”ì‹œì§€ ì°¾ê¸° ì‹¤íŒ¨ (human: {bool(human_msg)}, ai: {bool(ai_msg)})")
            
            # í‰ê°€ ì‹¤í–‰
            if human_msg and ai_msg:
                logger.info(f"[4. Eval Turn Guard] ===== í„´ {turn} í‰ê°€ ì‹œì‘ =====")
                logger.info(f"[4. Eval Turn Guard] ì‚¬ìš©ì ë©”ì‹œì§€: {human_msg[:100]}...")
                logger.info(f"[4. Eval Turn Guard] AI ì‘ë‹µ: {ai_msg[:100]}...")
                logger.info("")
                
                # í‰ê°€ ì‹¤í–‰ ë° ê²°ê³¼ ë°›ê¸°
                eval_result = await _evaluate_turn_sync(
                    session_id=session_id,
                    turn=turn,
                    human_message=human_msg,
                    ai_message=ai_msg,
                    problem_context=state.get("problem_context")
                )
                
                # í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
                logger.info("")
                logger.info("=" * 80)
                logger.info(f"[4. Eval Turn Guard] ===== í„´ {turn} í‰ê°€ ì™„ë£Œ âœ“ =====")
                
                if eval_result:
                    intent_type = eval_result.get("intent_type", "UNKNOWN")
                    turn_score = eval_result.get("turn_score", 0)
                    intent_confidence = eval_result.get("intent_confidence", 0.0)
                    rubrics = eval_result.get("rubrics", [])
                    comprehensive_reasoning = eval_result.get("comprehensive_reasoning", "")
                    
                    logger.info(f"[4. Eval Turn Guard] ğŸ“Š í„´ {turn} í‰ê°€ ê²°ê³¼ ìš”ì•½:")
                    logger.info(f"[4. Eval Turn Guard]   â€¢ ì˜ë„: {intent_type} (ì‹ ë¢°ë„: {intent_confidence:.2f})")
                    logger.info(f"[4. Eval Turn Guard]   â€¢ ì ìˆ˜: {turn_score:.2f}ì ")
                    
                    if rubrics:
                        logger.info(f"[4. Eval Turn Guard]   â€¢ ë£¨ë¸Œë¦­ í‰ê°€ ({len(rubrics)}ê°œ):")
                        for rubric in rubrics[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                            rubric_name = rubric.get("name", rubric.get("criterion", ""))
                            rubric_score = rubric.get("score", 0)
                            logger.info(f"[4. Eval Turn Guard]     - {rubric_name}: {rubric_score:.2f}ì ")
                        if len(rubrics) > 5:
                            logger.info(f"[4. Eval Turn Guard]     ... ì™¸ {len(rubrics) - 5}ê°œ")
                    
                    if comprehensive_reasoning:
                        reasoning_preview = comprehensive_reasoning[:200] + "..." if len(comprehensive_reasoning) > 200 else comprehensive_reasoning
                        logger.info(f"[4. Eval Turn Guard]   â€¢ í‰ê°€ ë‚´ìš©: {reasoning_preview}")
                else:
                    logger.warning(f"[4. Eval Turn Guard]   âš ï¸ í‰ê°€ ê²°ê³¼ ì •ë³´ ì—†ìŒ")
                
                logger.info("=" * 80)
                logger.info("")
            else:
                logger.error("")
                logger.error(f"[4. Eval Turn Guard] í„´ {turn} ë©”ì‹œì§€ ì¶”ì¶œ ì‹¤íŒ¨ - human: {bool(human_msg)}, ai: {bool(ai_msg)}")
                logger.error(f"[4. Eval Turn Guard] í„´ {turn} - í‰ê°€ ë¶ˆê°€ëŠ¥ âœ—")
                logger.error("")
        
        logger.info("")
        logger.info("-" * 80)
        logger.info(f"[4. Eval Turn Guard] âœ… ëª¨ë“  í„´ í‰ê°€ ì™„ë£Œ - session_id: {session_id}, í‰ê°€ ì™„ë£Œ: {len(turns_to_evaluate)}í„´")
        logger.info("-" * 80)
        logger.info("")
        
        # Redisì—ì„œ ìµœì‹  turn_logs ì¡°íšŒ (í‰ê°€ ê²°ê³¼ ë°˜ì˜)
        updated_turn_logs = await redis_client.get_all_turn_logs(session_id)
        
        # turn_logsë¥¼ turn_scoresë¡œ ë³€í™˜í•˜ì—¬ state ì—…ë°ì´íŠ¸
        # 6b ë…¸ë“œì—ì„œ {"turn_score": ...} í˜•ì‹ì„ ê¸°ëŒ€í•˜ë¯€ë¡œ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥
        turn_scores = {}
        for turn_key, turn_log in updated_turn_logs.items():
            if isinstance(turn_log, dict) and "prompt_evaluation_details" in turn_log:
                score = turn_log["prompt_evaluation_details"].get("score", 0)
                turn_scores[turn_key] = {
                    "turn_score": score  # 6b ë…¸ë“œê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€ê²½
                }
        
        logger.info(f"[4. Eval Turn Guard] ì™„ë£Œ - session_id: {session_id}, ìµœì¢… í„´ ë¡œê·¸ ê°œìˆ˜: {len(updated_turn_logs)}, turn_scores: {turn_scores}")
        logger.info("")
        logger.info("=" * 80)
        logger.info(f"[4. Eval Turn Guard] ì¢…ë£Œ")
        logger.info("=" * 80)
        logger.info("")
        
        return {
            "turn_scores": turn_scores,
            "updated_at": datetime.utcnow().isoformat(),
        }
        
    except Exception as e:
        logger.error("")
        logger.error(f"[4. Eval Turn Guard] ì˜¤ë¥˜ - session_id: {session_id}, error: {str(e)}", exc_info=True)
        logger.error("")
        return {
            "error_message": f"í„´ í‰ê°€ ê°€ë“œ ì˜¤ë¥˜: {str(e)}",
            "updated_at": datetime.utcnow().isoformat(),
        }


async def _evaluate_turn_sync(
    session_id: str,
    turn: int,
    human_message: str,
    ai_message: str,
    problem_context: Optional[Dict[str, Any]] = None
) -> Optional[Dict[str, Any]]:
    """
    íŠ¹ì • í„´ì„ ë™ê¸°ì ìœ¼ë¡œ í‰ê°€
    
    ì œì¶œ ì‹œ ëª¨ë“  í„´ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì‚¬ìš©
    """
    try:
        from app.domain.langgraph.subgraph_eval_turn import create_eval_turn_subgraph
        from app.domain.langgraph.states import EvalTurnState
        
        # Eval Turn SubGraph ìƒì„±
        eval_turn_subgraph = create_eval_turn_subgraph()
        
        # SubGraph ì…ë ¥ ì¤€ë¹„
        turn_state: EvalTurnState = {
            "session_id": session_id,
            "turn": turn,
            "human_message": human_message,
            "ai_message": ai_message,
            "problem_context": problem_context,  # ë¬¸ì œ ì •ë³´ ì „ë‹¬
            "is_guardrail_failed": False,
            "guardrail_message": None,
            "intent_type": None,
            "intent_confidence": 0.0,
            "rule_setting_eval": None,
            "generation_eval": None,
            "optimization_eval": None,
            "debugging_eval": None,
            "test_case_eval": None,
            "hint_query_eval": None,
            "follow_up_eval": None,
            "answer_summary": None,
            "turn_log": None,
            "turn_score": None,
        }
        
        # SubGraph ì‹¤í–‰ (ë™ê¸°)
        logger.info(f"[Eval Turn Sync] Eval Turn SubGraph ì‹¤í–‰ ì‹œì‘ - turn: {turn}")
        result = await eval_turn_subgraph.ainvoke(turn_state)
        logger.info(f"[Eval Turn Sync] Eval Turn SubGraph ì‹¤í–‰ ì™„ë£Œ - turn: {turn}")
        
        intent_type = result.get("intent_type", "UNKNOWN")
        intent_types = result.get("intent_types", [intent_type] if intent_type and intent_type != "UNKNOWN" else [])
        intent_confidence = result.get("intent_confidence", 0.0)
        turn_score = result.get("turn_score", 0)
        turn_log_data = result.get("turn_log", {})
        evaluations = turn_log_data.get("evaluations", {})
        detailed_feedback = turn_log_data.get("detailed_feedback", [])
        comprehensive_reasoning = turn_log_data.get("comprehensive_reasoning", "")
        
        # intent_typeì´ "UNKNOWN"ì´ë©´ intent_types[0] ì‚¬ìš©
        if intent_type == "UNKNOWN" and intent_types:
            intent_type = intent_types[0]
        
        # final_intent ì •ì˜ (JSON ì¶œë ¥ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë¨¼ì € ì •ì˜)
        final_intent = intent_type if intent_type != "UNKNOWN" else (intent_types[0] if intent_types else "UNKNOWN")
        
        logger.info(f"[Eval Turn Sync] í„´ {turn} í‰ê°€ ê²°ê³¼:")
        logger.info(f"[Eval Turn Sync]   - Intent Type: {intent_type}")
        logger.info(f"[Eval Turn Sync]   - Intent Types: {intent_types}")
        logger.info(f"[Eval Turn Sync]   - Intent Confidence: {intent_confidence}")
        logger.info(f"[Eval Turn Sync]   - Turn Score: {turn_score}")
        
        # LLM ì‘ë‹µ ìš”ì•½ ë¡œê·¸
        answer_summary = result.get("answer_summary", "")
        if answer_summary:
            logger.info(f"[Eval Turn Sync]   - Answer Summary: {answer_summary[:200]}...")
        
        # ìƒì„¸ í‰ê°€ ë‚´ìš© ë¡œê·¸ (ë£¨ë¸Œë¦­, ë¶„ì„ ë“±)
        logger.info(f"[Eval Turn Sync] ===== í„´ {turn} ìƒì„¸ í‰ê°€ ë‚´ìš© =====")
        
        # comprehensive_reasoning ë¡œê·¸
        if comprehensive_reasoning:
            logger.info(f"[Eval Turn Sync]   - ì¢…í•© ë¶„ì„:")
            logger.info(f"[Eval Turn Sync]     {comprehensive_reasoning[:500]}{'...' if len(comprehensive_reasoning) > 500 else ''}")
            
            # ì „ì²´ ë¶„ì„ í…ìŠ¤íŠ¸ JSON ì¶œë ¥ (ë°œí‘œìë£Œìš©)
            import json
            analysis_json = {
                "turn": turn,
                "intent": final_intent,
                "analysis_text": comprehensive_reasoning
            }
            logger.info("")
            logger.info(f"[Eval Turn Sync] ===== í„´ {turn} í‰ê°€ ë¶„ì„ í…ìŠ¤íŠ¸ (JSON) =====")
            logger.info(json.dumps(analysis_json, indent=4, ensure_ascii=False))
            logger.info("")
        
        # evaluations ë¡œê·¸ (ê° í‰ê°€ íƒ€ì…ë³„ ê²°ê³¼)
        if evaluations:
            logger.info(f"[Eval Turn Sync]   - í‰ê°€ íƒ€ì…ë³„ ê²°ê³¼:")
            for eval_key, eval_result in evaluations.items():
                if isinstance(eval_result, dict):
                    eval_score = eval_result.get("score", eval_result.get("average", 0))
                    eval_feedback = eval_result.get("final_reasoning", eval_result.get("feedback", ""))
                    logger.info(f"[Eval Turn Sync]     * {eval_key}: {eval_score:.2f}ì ")
                    if eval_feedback:
                        logger.info(f"[Eval Turn Sync]       {eval_feedback[:200]}{'...' if len(eval_feedback) > 200 else ''}")
        
        # detailed_feedback ë¡œê·¸
        if detailed_feedback:
            logger.info(f"[Eval Turn Sync]   - ìƒì„¸ í”¼ë“œë°± ({len(detailed_feedback)}ê°œ):")
            for idx, feedback in enumerate(detailed_feedback, 1):
                feedback_intent = feedback.get("intent", "UNKNOWN")
                feedback_score = feedback.get("score", 0)
                logger.info(f"[Eval Turn Sync]     [{idx}] Intent: {feedback_intent}, Score: {feedback_score:.2f}ì ")
                feedback_rubrics = feedback.get("rubrics", [])
                if feedback_rubrics:
                    for rubric in feedback_rubrics:
                        if isinstance(rubric, dict):
                            rubric_name = rubric.get("criterion", rubric.get("name", ""))
                            rubric_score = rubric.get("score", 0)
                            rubric_reasoning = rubric.get("reasoning", rubric.get("reason", ""))
                            logger.info(f"[Eval Turn Sync]       - {rubric_name}: {rubric_score:.2f}ì ")
                            if rubric_reasoning:
                                logger.info(f"[Eval Turn Sync]         ì´ìœ : {rubric_reasoning[:150]}{'...' if len(rubric_reasoning) > 150 else ''}")
        
        # weights ì •ë³´ ê°€ì ¸ì˜¤ê¸° (intent_typeì„ ëŒ€ë¬¸ìë¡œ ë³€í™˜)
        from app.domain.langgraph.nodes.turn_evaluator.weights import get_weights_for_intent
        # intent_typeì´ ì†Œë¬¸ì í˜•ì‹("hint_or_query")ì´ë©´ ëŒ€ë¬¸ìë¡œ ë³€í™˜
        intent_for_weights = intent_type.upper().replace("-", "_") if intent_type else "UNKNOWN"
        weights = get_weights_for_intent(intent_for_weights)
        
        # ìƒì„¸ ë£¨ë¸Œë¦­ ì •ë³´ ì¶”ì¶œ
        # 1ìˆœìœ„: detailed_feedbackì—ì„œ primary intentì˜ rubrics ì‚¬ìš©
        # 2ìˆœìœ„: evaluationsì—ì„œ primary intentì˜ rubrics ì‚¬ìš©
        # 3ìˆœìœ„: resultì—ì„œ ì§ì ‘ ì¶”ì¶œ
        detailed_rubrics = []
        
        # detailed_feedbackì—ì„œ primary intentì˜ rubrics ì¶”ì¶œ
        if detailed_feedback:
            # intent_typeì— í•´ë‹¹í•˜ëŠ” í”¼ë“œë°± ì°¾ê¸°
            primary_feedback = None
            for feedback in detailed_feedback:
                feedback_intent = feedback.get("intent", "")
                # intent_typeê³¼ ë§¤ì¹­ (ì˜ˆ: "HINT_OR_QUERY" -> "hint_query_eval")
                if intent_type and (intent_type in feedback_intent.upper() or feedback_intent.upper() in intent_type):
                    primary_feedback = feedback
                    break
            
            # ë§¤ì¹­ë˜ëŠ” ê²ƒì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ í”¼ë“œë°± ì‚¬ìš©
            if not primary_feedback and detailed_feedback:
                primary_feedback = detailed_feedback[0]
            
            if primary_feedback:
                feedback_rubrics = primary_feedback.get("rubrics", [])
                for rubric in feedback_rubrics:
                    if isinstance(rubric, dict):
                        detailed_rubrics.append({
                            "name": rubric.get("criterion", rubric.get("name", "")),
                            "score": rubric.get("score", 0.0),
                            "reasoning": rubric.get("reasoning", rubric.get("reason", "í‰ê°€ ì—†ìŒ")),
                            "criterion": rubric.get("criterion", rubric.get("name", ""))  # í˜¸í™˜ì„± ìœ ì§€
                        })
        
        # detailed_rubricsê°€ ì—†ìœ¼ë©´ evaluationsì—ì„œ ì¶”ì¶œ
        if not detailed_rubrics and evaluations:
            # intent_typeì„ eval_keyë¡œ ë³€í™˜ ì‹œë„
            intent_to_eval_key = {
                "GENERATION": "generation_eval",
                "OPTIMIZATION": "optimization_eval",
                "DEBUGGING": "debugging_eval",
                "TEST_CASE": "test_case_eval",
                "HINT_OR_QUERY": "hint_query_eval",
                "FOLLOW_UP": "follow_up_eval",
                "RULE_SETTING": "rule_setting_eval",
            }
            
            eval_key = intent_to_eval_key.get(intent_type)
            primary_eval = evaluations.get(eval_key) if eval_key else None
            
            if not primary_eval and evaluations:
                # ì²« ë²ˆì§¸ í‰ê°€ ê²°ê³¼ ì‚¬ìš©
                primary_eval = list(evaluations.values())[0] if evaluations else None
            
            if primary_eval and isinstance(primary_eval, dict):
                eval_rubrics = primary_eval.get("rubrics", [])
                for rubric in eval_rubrics:
                    if isinstance(rubric, dict):
                        detailed_rubrics.append({
                            "name": rubric.get("criterion", rubric.get("name", "")),
                            "score": rubric.get("score", 0.0),
                            "reasoning": rubric.get("reasoning", rubric.get("reason", "í‰ê°€ ì—†ìŒ")),
                            "criterion": rubric.get("criterion", rubric.get("name", ""))  # í˜¸í™˜ì„± ìœ ì§€
                        })
        
        # detailed_rubricsê°€ ì—¬ì „íˆ ì—†ìœ¼ë©´ resultì—ì„œ ì§ì ‘ ì¶”ì¶œ (fallback)
        if not detailed_rubrics:
            eval_mapping = {
                "rule_setting_eval": "ê·œì¹™ ì„¤ì • (Rules)",
                "generation_eval": "ì½”ë“œ ìƒì„± (Generation)",
                "optimization_eval": "ìµœì í™” (Optimization)",
                "debugging_eval": "ë””ë²„ê¹… (Debugging)",
                "test_case_eval": "í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ (Test Case)",
                "hint_query_eval": "íŒíŠ¸/ì§ˆì˜ (Hint/Query)",
                "follow_up_eval": "í›„ì† ì‘ë‹µ (Follow-up)"
            }
            
            for eval_key, criterion_name in eval_mapping.items():
                eval_result = result.get(eval_key)
                if eval_result and isinstance(eval_result, dict):
                    # eval_resultì˜ rubrics ì‚¬ìš© (ìƒì„¸ ì •ë³´)
                    eval_rubrics = eval_result.get("rubrics", [])
                    if eval_rubrics:
                        for rubric in eval_rubrics:
                            if isinstance(rubric, dict):
                                detailed_rubrics.append({
                                    "name": rubric.get("criterion", criterion_name),
                                    "score": rubric.get("score", eval_result.get("average", 0)),
                                    "reasoning": rubric.get("reasoning", rubric.get("reason", "í‰ê°€ ì—†ìŒ")),
                                    "criterion": rubric.get("criterion", criterion_name)  # í˜¸í™˜ì„± ìœ ì§€
                                })
                    else:
                        # rubricsê°€ ì—†ìœ¼ë©´ ê°„ë‹¨í•œ í˜•ì‹
                        detailed_rubrics.append({
                            "name": criterion_name,
                            "score": eval_result.get("average", 0),
                            "reasoning": eval_result.get("final_reasoning", eval_result.get("feedback", "í‰ê°€ ì—†ìŒ")),
                            "criterion": criterion_name  # í˜¸í™˜ì„± ìœ ì§€
                        })
        
        # ìƒì„¸ turn_log êµ¬ì¡° ìƒì„± (ì¤‘ë³µ ì œê±°)
        # final_intentëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì •ì˜ë¨ (288ë²ˆ ì¤„)
        
        # detailed_rubrics ë¡œê·¸ (ìµœì¢… ë£¨ë¸Œë¦­ ì •ë³´)
        if detailed_rubrics:
            logger.info(f"[Eval Turn Sync]   - ìµœì¢… ë£¨ë¸Œë¦­ í‰ê°€ ({len(detailed_rubrics)}ê°œ):")
            for rubric in detailed_rubrics:
                rubric_name = rubric.get("name", rubric.get("criterion", ""))
                rubric_score = rubric.get("score", 0)
                rubric_reasoning = rubric.get("reasoning", "")
                logger.info(f"[Eval Turn Sync]     * {rubric_name}: {rubric_score:.2f}ì ")
                if rubric_reasoning:
                    logger.info(f"[Eval Turn Sync]       {rubric_reasoning[:150]}{'...' if len(rubric_reasoning) > 150 else ''}")
        
        # JSON í˜•ì‹ìœ¼ë¡œ ì ìˆ˜ì™€ ê°€ì¤‘ì¹˜ ì¶œë ¥ (ë°œí‘œìë£Œìš©)
        import json
        if detailed_rubrics and weights:
            # ì˜ë„ íƒ€ì…ì„ ëŒ€ë¬¸ìë¡œ ë³€í™˜ (ì˜ˆ: "hint_or_query" -> "HINT_OR_QUERY")
            intent_display = final_intent.upper().replace("-", "_") if final_intent else "UNKNOWN"
            
            # ë£¨ë¸Œë¦­ ì´ë¦„ ë§¤í•‘ ì‚¬ìš© (weights.pyì˜ RUBRIC_NAME_MAP)
            from app.domain.langgraph.nodes.turn_evaluator.weights import RUBRIC_NAME_MAP
            
            # ë£¨ë¸Œë¦­ ì ìˆ˜ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ê°€ì¤‘ì¹˜ í‚¤ ìˆœì„œëŒ€ë¡œ)
            rubric_scores = {}
            for rubric in detailed_rubrics:
                rubric_name = rubric.get("name", rubric.get("criterion", ""))
                rubric_score = rubric.get("score", 0)
                
                # RUBRIC_NAME_MAPì„ ì‚¬ìš©í•˜ì—¬ ë£¨ë¸Œë¦­ ì´ë¦„ì„ ê°€ì¤‘ì¹˜ í‚¤ë¡œ ë³€í™˜
                weight_key = RUBRIC_NAME_MAP.get(rubric_name)
                
                # ë§¤í•‘ì´ ì—†ìœ¼ë©´ ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
                if not weight_key:
                    rubric_lower = rubric_name.lower()
                    for map_key, map_value in RUBRIC_NAME_MAP.items():
                        if map_key.lower() in rubric_lower or any(word in rubric_lower for word in map_key.split()):
                            weight_key = map_value
                            break
                
                # ì—¬ì „íˆ ì—†ìœ¼ë©´ ì§ì ‘ ë§¤ì¹­ ì‹œë„
                if not weight_key:
                    if "ëª…í™•ì„±" in rubric_name or "clarity" in rubric_name.lower():
                        weight_key = "clarity"
                    elif "ë¬¸ì œ ì ì ˆì„±" in rubric_name or "problem" in rubric_name.lower() or "relevance" in rubric_name.lower():
                        weight_key = "problem_relevance"
                    elif "ì˜ˆì‹œ" in rubric_name or "example" in rubric_name.lower():
                        weight_key = "examples"
                    elif "ê·œì¹™" in rubric_name or "rule" in rubric_name.lower():
                        weight_key = "rules"
                    elif "ë¬¸ë§¥" in rubric_name or "context" in rubric_name.lower():
                        weight_key = "context"
                
                if weight_key:
                    rubric_scores[weight_key] = round(rubric_score, 2)
            
            # ê°€ì¤‘ì¹˜ì— ìˆëŠ” ëª¨ë“  í‚¤ë¥¼ ì ìˆ˜ì—ë„ í¬í•¨ (ì ìˆ˜ê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì„¤ì •)
            for weight_key in weights.keys():
                if weight_key not in rubric_scores:
                    rubric_scores[weight_key] = 0.0
            
            # ì ìˆ˜ JSON ì¶œë ¥ (ê°€ì¤‘ì¹˜ í‚¤ ìˆœì„œëŒ€ë¡œ ì •ë ¬)
            if rubric_scores:
                # ê°€ì¤‘ì¹˜ í‚¤ ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ ë”•ì…”ë„ˆë¦¬ ìƒì„±
                ordered_scores = {key: rubric_scores.get(key, 0.0) for key in weights.keys()}
                scores_json = {intent_display: ordered_scores}
                logger.info("")
                logger.info(f"[Eval Turn Sync] ===== í„´ {turn} í‰ê°€ ì ìˆ˜ (JSON) =====")
                logger.info(json.dumps(scores_json, indent=4, ensure_ascii=False))
                logger.info("")
            
            # ê°€ì¤‘ì¹˜ JSON ì¶œë ¥
            weights_json = {intent_display: weights}
            logger.info(f"[Eval Turn Sync] ===== í„´ {turn} ê°€ì¤‘ì¹˜ (JSON) =====")
            logger.info("## Weight")
            logger.info(json.dumps(weights_json, indent=4, ensure_ascii=False))
            logger.info("")
        
        # weights ë¡œê·¸ (ê¸°ì¡´ í˜•ì‹ ìœ ì§€)
        if weights:
            logger.info(f"[Eval Turn Sync]   - ê°€ì¤‘ì¹˜:")
            for weight_key, weight_value in weights.items():
                logger.info(f"[Eval Turn Sync]     * {weight_key}: {weight_value}")
        
        logger.info(f"[Eval Turn Sync] ===== í„´ {turn} ìƒì„¸ í‰ê°€ ë‚´ìš© ì¢…ë£Œ =====")
        
        detailed_turn_log = {
            "turn_number": turn,
            "user_prompt_summary": human_message[:200] + "..." if len(human_message) > 200 else human_message,
            "prompt_evaluation_details": {
                "intent": final_intent,  # UNKNOWN ëŒ€ì‹  ì‹¤ì œ intent ì‚¬ìš©
                "intent_types": intent_types,
                "intent_confidence": intent_confidence,
                "score": turn_score,
                "rubrics": detailed_rubrics,  # ìƒì„¸ ë£¨ë¸Œë¦­ ì •ë³´ (ì¤‘ë³µ ì œê±°)
                "weights": weights,  # ê°€ì¤‘ì¹˜ ì •ë³´ (ì˜¬ë°”ë¥¸ intentë¡œ ê°€ì ¸ì˜¨ ê°’)
                "final_reasoning": comprehensive_reasoning or result.get("answer_summary", "ì¬í‰ê°€ ì™„ë£Œ")
            },
            "llm_answer_summary": result.get("answer_summary", ""),
            "llm_answer_reasoning": comprehensive_reasoning or (detailed_rubrics[0].get("reasoning", "") if detailed_rubrics else "í‰ê°€ ì—†ìŒ"),
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Redisì— ìƒì„¸ turn_log ì €ì¥
        await redis_client.save_turn_log(session_id, turn, detailed_turn_log)
        
        # PostgreSQLì— í‰ê°€ ê²°ê³¼ ì €ì¥
        try:
            from app.infrastructure.persistence.session import get_db_context
            from app.application.services.evaluation_storage_service import EvaluationStorageService
            
            # session_idë¥¼ PostgreSQL idë¡œ ë³€í™˜ (Redis session_id: "session_123" -> PostgreSQL id: 123)
            postgres_session_id = int(session_id.replace("session_", "")) if session_id.startswith("session_") else None
            
            if postgres_session_id:
                async with get_db_context() as db:
                    storage_service = EvaluationStorageService(db)
                    
                    # turn_logë¥¼ aggregate_turn_log í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ìƒì„¸ ì •ë³´ í¬í•¨)
                    turn_log_for_storage = {
                        "prompt_evaluation_details": detailed_turn_log.get("prompt_evaluation_details", {}),
                        "comprehensive_reasoning": comprehensive_reasoning or detailed_turn_log.get("llm_answer_reasoning", ""),
                        "intent_types": intent_types,
                        "intent_confidence": intent_confidence,
                        "evaluations": evaluations,  # ì „ì²´ í‰ê°€ ê²°ê³¼ (ìƒì„¸ ì •ë³´ í¬í•¨)
                        "detailed_feedback": detailed_feedback,  # ìƒì„¸ í”¼ë“œë°±
                        "turn_score": turn_score,
                        "is_guardrail_failed": turn_log_data.get("is_guardrail_failed", False),
                        "guardrail_message": turn_log_data.get("guardrail_message"),
                    }
                    
                    pg_evaluation = await storage_service.save_turn_evaluation(
                        session_id=postgres_session_id,
                        turn=turn,
                        turn_log=turn_log_for_storage
                    )
                    
                    if pg_evaluation:
                        await db.commit()
                        logger.info(
                            f"[Eval Turn Sync] PostgreSQL í„´ í‰ê°€ ì €ì¥ ì™„ë£Œ - "
                            f"session_id: {postgres_session_id}, turn: {turn}"
                        )
                    else:
                        # ì €ì¥ ì‹¤íŒ¨ (None ë°˜í™˜)
                        await db.rollback()
                        logger.warning(
                            f"[Eval Turn Sync] PostgreSQL í„´ í‰ê°€ ì €ì¥ ì‹¤íŒ¨ (None ë°˜í™˜) - "
                            f"session_id: {postgres_session_id}, turn: {turn}"
                        )
        except Exception as pg_error:
            # PostgreSQL ì €ì¥ ì‹¤íŒ¨í•´ë„ RedisëŠ” ì €ì¥ë˜ì—ˆìœ¼ë¯€ë¡œ ê²½ê³ ë§Œ
            logger.warning(
                f"[Eval Turn Sync] PostgreSQL í„´ í‰ê°€ ì €ì¥ ì‹¤íŒ¨ (RedisëŠ” ì €ì¥ë¨) - "
                f"session_id: {session_id}, turn: {turn}, error: {str(pg_error)}"
            )
                    
        logger.info(f"[Eval Turn Sync] í„´ {turn} í‰ê°€ ì €ì¥ ì™„ë£Œ - session_id: {session_id}, score: {turn_score}")
        
        # í‰ê°€ ê²°ê³¼ ë°˜í™˜ (ìš”ì•½ ì •ë³´ í¬í•¨)
        # resultëŠ” Eval Turn SubGraphì˜ ê²°ê³¼ (dict), answer_summaryëŠ” ì´ë¯¸ ì¶”ì¶œë¨
        return {
            "intent_type": final_intent,
            "intent_types": intent_types,
            "intent_confidence": intent_confidence,
            "turn_score": turn_score,
            "rubrics": detailed_rubrics,
            "comprehensive_reasoning": comprehensive_reasoning or answer_summary,
            "answer_summary": answer_summary
        }
        
    except Exception as e:
        logger.error(f"[Eval Turn Sync] í„´ {turn} í‰ê°€ ì‹¤íŒ¨ - session_id: {session_id}, error: {str(e)}", exc_info=True)
        return None



