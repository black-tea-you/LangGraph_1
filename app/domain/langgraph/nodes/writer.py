"""
ë…¸ë“œ 3: Writer LLM
AI ë‹µë³€ ìƒì„±
"""

from typing import Dict, Any
from datetime import datetime

from langchain_google_genai import ChatGoogleGenerativeAI

from app.domain.langgraph.states import MainGraphState
from app.core.config import settings
from app.infrastructure.persistence.models.enums import WriterResponseStatus


def get_llm():
    """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return ChatGoogleGenerativeAI(
        model=settings.DEFAULT_LLM_MODEL,
        google_api_key=settings.GEMINI_API_KEY,
        temperature=settings.LLM_TEMPERATURE,
        max_tokens=settings.LLM_MAX_TOKENS,
    )


async def writer_llm(state: MainGraphState) -> Dict[str, Any]:
    """
    AI ë‹µë³€ ìƒì„±
    
    ì—­í• :
    - ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ ì½”ë“œ ì‘ì„±
    - íŒíŠ¸ ì œê³µ
    - ë””ë²„ê¹… ë„ì›€
    - ì„¤ëª… ì œê³µ
    """
    import logging
    logger = logging.getLogger(__name__)
    
    human_message = state.get("human_message", "")
    messages = state.get("messages", [])
    memory_summary = state.get("memory_summary", "")
    is_guardrail_failed = state.get("is_guardrail_failed", False)
    guardrail_message = state.get("guardrail_message", "")
    
    logger.info(f"[Writer LLM] ë‹µë³€ ìƒì„± ì‹œì‘ - message: {human_message[:100]}..., guardrail_failed: {is_guardrail_failed}")
    
    llm = get_llm()
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê°€ë“œë ˆì¼ ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¦„)
    if is_guardrail_failed:
        # ê°€ë“œë ˆì¼ ìœ„ë°˜ ì‹œ: êµìœ¡ì  ê±°ì ˆ ë©”ì‹œì§€
        system_prompt = f"""ë‹¹ì‹ ì€ AI ì½”ë”© í…ŒìŠ¤íŠ¸ì˜ ë³´ì•ˆ ê´€ë¦¬ì(Gatekeeper)ì…ë‹ˆë‹¤.

# ğŸ›¡ï¸ ìƒí™©
ì‚¬ìš©ìì˜ ìš”ì²­ì´ í…ŒìŠ¤íŠ¸ ì •ì±…ì— ìœ„ë°˜ë˜ì—ˆìŠµë‹ˆë‹¤.
ìœ„ë°˜ ì´ìœ : {guardrail_message or "ë¶€ì ì ˆí•œ ìš”ì²­"}

# âœ‹ ê±°ì ˆ ë©”ì‹œì§€ ìƒì„± ê·œì¹™
1. **ì •ì¤‘í•˜ê²Œ ê±°ì ˆ**: "í•´ë‹¹ ìš”ì²­ì€ í…ŒìŠ¤íŠ¸ ì •ì±…ìƒ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
2. **ì´ìœ  ê°„ë‹¨ ì„¤ëª…**: ì™œ ê±°ì ˆí•˜ëŠ”ì§€ 1-2ì¤„ë¡œ ì„¤ëª…
3. **ëŒ€ì•ˆ ì œì‹œ**: ëŒ€ì‹  **ê°œë…(Concept)** ìˆ˜ì¤€ì—ì„œ í•™ìŠµ ë°©í–¥ ì œì‹œ
4. **ì†Œí¬ë¼í…ŒìŠ¤ì‹ ë°˜ë¬¸**: ì§ˆë¬¸ì„ ë˜ì ¸ ìŠ¤ìŠ¤ë¡œ ìƒê°í•˜ê²Œ ìœ ë„

# ğŸ“œ ì‘ë‹µ í˜•ì‹ ì˜ˆì‹œ
```
ì£„ì†¡í•©ë‹ˆë‹¤ë§Œ, í•´ë‹¹ ìš”ì²­ì€ ë¬¸ì œì˜ ì •ë‹µê³¼ ì§ê²°ë˜ì–´ ìˆì–´ ì§ì ‘ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.

ëŒ€ì‹ , ë‹¤ìŒ ê°œë…ë“¤ì„ ê³µë¶€í•´ë³´ì‹œëŠ” ê±´ ì–´ë–¨ê¹Œìš”?
- ë¹„íŠ¸ë§ˆìŠ¤í‚¹ìœ¼ë¡œ ìƒíƒœ í‘œí˜„í•˜ê¸°
- ë™ì  ê³„íšë²•ì˜ ë©”ëª¨ì´ì œì´ì…˜

ìŠ¤ìŠ¤ë¡œ ìƒê°í•´ë³´ì„¸ìš”: "ëª¨ë“  ë„ì‹œë¥¼ ë°©ë¬¸í–ˆëŠ”ì§€ ì–´ë–»ê²Œ í™•ì¸í•  ìˆ˜ ìˆì„ê¹Œìš”?"
```

**í†¤**: ì—„ê²©í•˜ì§€ë§Œ êµìœ¡ì , ê²©ë ¤í•˜ëŠ” íƒœë„
"""
    else:
        # ì •ìƒ ìš”ì²­ ì‹œ: ì†Œí¬ë¼í…ŒìŠ¤ì‹ íŠœí„°
        system_prompt = """ë‹¹ì‹ ì€ ì†Œí¬ë¼í…ŒìŠ¤ì‹ êµìœ¡ë²•ì„ ì§€í–¥í•˜ëŠ” AI ì½”ë”© íŠœí„°ì…ë‹ˆë‹¤.

# ğŸ¯ ì—­í• 
ì‚¬ìš©ìì˜ ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œ í•´ê²°ì„ ë•ë˜, **ì •ë‹µì„ ì§ì ‘ ì£¼ì§€ ì•Šê³ ** ìŠ¤ìŠ¤ë¡œ ê¹¨ë‹«ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.

# âœï¸ ë‹µë³€ ê·œì¹™
1. **ì •ë‹µ ì½”ë“œ ì§€ì–‘**: í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ë¡œì§ì€ ì§ì ‘ ì£¼ì§€ ì•ŠìŒ
2. **ë‹µë³€ í˜•ì‹**:
   - `[Syntax]`: ìˆœìˆ˜ ë¬¸ë²• ì˜ˆì‹œ (ë¬¸ì œì™€ ë¬´ê´€)
   - `[Concept]`: ê°œë…ì  ì„¤ëª…
   - `[Roadmap]`: ë‹¨ê³„ë³„ ì ‘ê·¼ë²•
   - `[Question]`: ë°˜ë¬¸ìœ¼ë¡œ ìœ ë„

3. **ì˜ˆì‹œ ì½”ë“œ**: ë¬¸ì œì™€ ì§ì ‘ ê´€ë ¨ ì—†ëŠ” ì¼ë°˜ì  ìƒí™©ë§Œ
   ```python
   # ë¹„íŠ¸ ì—°ì‚° ì˜ˆì‹œ
   visited = 0
   visited |= (1 << 3)  # 3ë²ˆ ë°©ë¬¸ í‘œì‹œ
   ```

4. **í†¤**: ì¹œì ˆí•˜ê³  ê²©ë ¤í•˜ë˜, ìŠ¤ìŠ¤ë¡œ ìƒê°í•˜ë„ë¡ ìœ ë„

# ê·œì¹™
1. ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œì™€ ì„¤ëª… ì œê³µ
2. ì ì ˆí•œ ì£¼ì„ í¬í•¨
3. íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ ê¶Œì¥
4. ì—ì§€ ì¼€ì´ìŠ¤ ê³ ë ¤
"""
    
    if memory_summary:
        system_prompt += f"\n\nì´ì „ ëŒ€í™” ìš”ì•½:\n{memory_summary}"
    
    # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ êµ¬ì„±
    chat_messages = [{"role": "system", "content": system_prompt}]
    
    # ìµœê·¼ ë©”ì‹œì§€ ì¶”ê°€ (ìµœëŒ€ 10ê°œ)
    recent_messages = messages[-10:] if len(messages) > 10 else messages
    for msg in recent_messages:
        if hasattr(msg, 'content'):
            role = getattr(msg, 'type', 'user')
            if role == 'human':
                role = 'user'
            elif role == 'ai':
                role = 'assistant'
            chat_messages.append({"role": role, "content": msg.content})
    
    # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€
    chat_messages.append({"role": "user", "content": human_message})
    
    try:
        response = await llm.ainvoke(chat_messages)
        ai_content = response.content
        
        logger.info(f"[Writer LLM] ë‹µë³€ ìƒì„± ì„±ê³µ - ê¸¸ì´: {len(ai_content)} ë¬¸ì")
        
        # í˜„ì¬ í„´ ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸°
        current_turn = state.get("current_turn", 0)
        session_id = state.get("session_id", "unknown")
        
        # ê¸°ì¡´ messages ë°°ì—´ ê¸¸ì´ í™•ì¸ (ìƒˆ ë©”ì‹œì§€ ì¸ë±ìŠ¤ ê³„ì‚°ìš©)
        existing_messages = state.get("messages", [])
        start_msg_idx = len(existing_messages)
        end_msg_idx = start_msg_idx + 1
        
        # Redisì— í„´-ë©”ì‹œì§€ ë§¤í•‘ ì €ì¥
        try:
            from app.infrastructure.cache.redis_client import redis_client
            import asyncio
            
            # ë¹„ë™ê¸°ë¡œ í„´ ë§¤í•‘ ì €ì¥ (ì‹¤íŒ¨í•´ë„ ë©”ì¸ í”Œë¡œìš° ì¤‘ë‹¨ ì•ˆ í•¨)
            asyncio.create_task(
                redis_client.save_turn_mapping(
                    session_id=session_id,
                    turn=current_turn,
                    start_msg_idx=start_msg_idx,
                    end_msg_idx=end_msg_idx
                )
            )
            logger.info(f"[Writer LLM] í„´ ë§¤í•‘ ì €ì¥ ì‹œì‘ - turn: {current_turn}, indices: [{start_msg_idx}, {end_msg_idx}]")
        except Exception as e:
            logger.warning(f"[Writer LLM] í„´ ë§¤í•‘ ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œ): {str(e)}")
        
        # messages ë°°ì—´ì— turn ì •ë³´ í¬í•¨ (ë°±ê·¸ë¼ìš´ë“œ 4ë²ˆ í‰ê°€ë¥¼ ìœ„í•´)
        new_messages = [
            {
                "turn": current_turn,
                "role": "user",
                "content": human_message,
                "timestamp": datetime.utcnow().isoformat()
            },
            {
                "turn": current_turn,
                "role": "assistant", 
                "content": ai_content,
                "timestamp": datetime.utcnow().isoformat()
            }
        ]
        
        return {
            "ai_message": ai_content,
            "messages": new_messages,
            "writer_status": WriterResponseStatus.SUCCESS.value,
            "writer_error": None,
            "updated_at": datetime.utcnow().isoformat(),
        }
        
    except Exception as e:
        logger.error(f"[Writer LLM] ì—ëŸ¬ ë°œìƒ: {str(e)}", exc_info=True)
        error_msg = str(e).lower()
        
        # ì—ëŸ¬ ìœ í˜• ë¶„ë¥˜
        if "rate" in error_msg or "quota" in error_msg:
            status = WriterResponseStatus.FAILED_RATE_LIMIT.value
            logger.warning(f"[Writer LLM] Rate limit ì´ˆê³¼")
        elif "context" in error_msg or "token" in error_msg:
            status = WriterResponseStatus.FAILED_THRESHOLD.value
            logger.warning(f"[Writer LLM] í† í° ì„ê³„ê°’ ì´ˆê³¼")
        else:
            status = WriterResponseStatus.FAILED_TECHNICAL.value
            logger.error(f"[Writer LLM] ê¸°ìˆ ì  ì˜¤ë¥˜: {str(e)}")
        
        return {
            "ai_message": None,
            "writer_status": status,
            "writer_error": str(e),
            "error_message": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "updated_at": datetime.utcnow().isoformat(),
        }


