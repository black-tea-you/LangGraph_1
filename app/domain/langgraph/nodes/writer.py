"""
ë…¸ë“œ 3: Writer LLM
AI ë‹µë³€ ìƒì„± (Runnable & Chain êµ¬ì¡°)
"""

from typing import Dict, Any, Optional
from datetime import datetime

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_vertexai import ChatVertexAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from app.domain.langgraph.states import MainGraphState
from app.core.config import settings
from app.infrastructure.persistence.models.enums import WriterResponseStatus
from app.domain.langgraph.middleware import wrap_chain_with_middleware
from app.domain.langgraph.utils.token_tracking import extract_token_usage, accumulate_tokens


def get_llm():
    """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (Vertex AI ë˜ëŠ” AI Studio)"""
    if settings.USE_VERTEX_AI:
        # Vertex AI ì‚¬ìš© (GCP í¬ë ˆë”§ ì‚¬ìš©)
        import json
        from google.oauth2 import service_account
        
        credentials = None
        if settings.GOOGLE_SERVICE_ACCOUNT_JSON:
            service_account_info = json.loads(settings.GOOGLE_SERVICE_ACCOUNT_JSON)
            credentials = service_account.Credentials.from_service_account_info(
                service_account_info
            )
        
        return ChatVertexAI(
            model=settings.DEFAULT_LLM_MODEL,
            project=settings.GOOGLE_PROJECT_ID,
            location=settings.GOOGLE_LOCATION,
            credentials=credentials,
            temperature=settings.LLM_TEMPERATURE,
            max_output_tokens=settings.LLM_MAX_TOKENS,
        )
    else:
        # AI Studio ì‚¬ìš© (API Key ë°©ì‹, Free Tier)
        return ChatGoogleGenerativeAI(
            model=settings.DEFAULT_LLM_MODEL,
            google_api_key=settings.GEMINI_API_KEY,
            temperature=settings.LLM_TEMPERATURE,
            max_output_tokens=settings.LLM_MAX_TOKENS,
        )


# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
GUARDRAIL_SYSTEM_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ 'ë°”ì´ë¸Œì½”ë”©'ì˜ **AI ì‹œí—˜ ê°ë…ê´€(AI Test Proctor)**ì…ë‹ˆë‹¤.

# ğŸ›¡ï¸ ìƒí™©
ì‚¬ìš©ìì˜ ìš”ì²­ì´ ì‹œí—˜ ê·œì •ì— ìœ„ë°˜ë˜ì—ˆìŠµë‹ˆë‹¤.
ìœ„ë°˜ ì´ìœ : {guardrail_message}

# âœ‹ ê±°ì ˆ ë° ëŒ€ì•ˆ ì œì‹œ ê·œì¹™
1. **ê±°ì ˆ ë©”ì‹œì§€**: "í•´ë‹¹ ìš”ì²­ì€ ì‹œí—˜ ê·œì •ìƒ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
2. **ì´ìœ  ê°„ë‹¨ ì„¤ëª…**: ì™œ ê±°ì ˆí•˜ëŠ”ì§€ 1ì¤„ë¡œ ì„¤ëª…
3. **í—ˆìš©ëœ ë„ì›€ ì—­ì œì•ˆ**: ì‚¬ìš©ìê°€ ì–´ë–¤ í‚¤ì›Œë“œë¥¼ ì¨ì•¼ í• ì§€ ëª°ë¼ë„, AIê°€ ì„ íƒì§€ë¥¼ ì œì‹œ
   - **ì¸í„°í˜ì´ìŠ¤(Interface) ìš”ì²­**: "í•¨ìˆ˜ ì •ì˜ë§Œ ì œê³µ ê°€ëŠ¥í•©ë‹ˆë‹¤. 'í•¨ìˆ˜ ê»ë°ê¸°ë§Œ ë§Œë“¤ì–´ì¤˜' ë˜ëŠ” 'ì¸í„°í˜ì´ìŠ¤ë§Œ ì•Œë ¤ì¤˜'ë¡œ ìš”ì²­í•´ì£¼ì„¸ìš”."
   - **ì˜ì‚¬ ì½”ë“œ(Pseudo-code) ìš”ì²­**: "ì˜ì‚¬ ì½”ë“œë¡œ ì œê³µ ê°€ëŠ¥í•©ë‹ˆë‹¤. 'ì˜ì‚¬ ì½”ë“œë¡œ ë³´ì—¬ì¤˜' ë˜ëŠ” 'ë¡œì§ íë¦„ë§Œ ì•Œë ¤ì¤˜'ë¡œ ìš”ì²­í•´ì£¼ì„¸ìš”."
   - **ì¼ë°˜ ê°œë… ì§ˆë¬¸**: "ì¼ë°˜ì ì¸ ê°œë… ì„¤ëª…ì€ ê°€ëŠ¥í•©ë‹ˆë‹¤. 'ë¹„íŠ¸ë§ˆìŠ¤í‚¹ì´ ë­”ê°€ìš”?' ë˜ëŠ” 'ë™ì  ê³„íšë²•ì˜ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”'ë¡œ ìš”ì²­í•´ì£¼ì„¸ìš”."

# ğŸ“œ ì‘ë‹µ í˜•ì‹ ì˜ˆì‹œ
```
í•´ë‹¹ ìš”ì²­ì€ ì‹œí—˜ ê·œì •ìƒ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì •ë‹µ ì½”ë“œëŠ” ì§ì ‘ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ëŒ€ì‹ , ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ìš”ì²­í•˜ì‹œë©´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- **ì¸í„°í˜ì´ìŠ¤ ì œê³µ**: "í•¨ìˆ˜ ê»ë°ê¸°ë§Œ ë§Œë“¤ì–´ì¤˜" ë˜ëŠ” "ì…ì¶œë ¥ êµ¬ì¡°ë§Œ ì¡ì•„ì¤˜"
- **ì˜ì‚¬ ì½”ë“œ ì œê³µ**: "ë¡œì§ íë¦„ë§Œ ì˜ì‚¬ ì½”ë“œë¡œ ë³´ì—¬ì¤˜" ë˜ëŠ” "ì˜ì‚¬ ì½”ë“œë¡œ ì„¤ëª…í•´ì¤˜"
- **ì¼ë°˜ ê°œë… ì„¤ëª…**: "ë¹„íŠ¸ë§ˆìŠ¤í‚¹ì´ ë­”ê°€ìš”?" ë˜ëŠ” "ë™ì  ê³„íšë²•ì˜ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
```

**í†¤**: ê±´ì¡°í•˜ê³  ê°ê´€ì , ì‚¬ë¬´ì  ì–´ì¡°
"""

def create_normal_system_prompt(
    status: str,
    guide_strategy: str,
    keywords: str,
    memory_summary: str,
    problem_context: Optional[Dict[str, Any]] = None,
    is_code_generation_request: bool = False
) -> str:
    """
    Writer LLM ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ë¬¸ì œ ì •ë³´ í¬í•¨)
    
    Args:
        status: ì•ˆì „ ìƒíƒœ (SAFE)
        guide_strategy: ê°€ì´ë“œ ì „ëµ (SYNTAX_GUIDE | LOGIC_HINT | ROADMAP | GENERATION | FULL_CODE_ALLOWED)
            - GENERATION: ì¸í„°í˜ì´ìŠ¤ë§Œ ì œê³µ (í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ + pass)
            - FULL_CODE_ALLOWED: ë§¥ë½ ê¸°ë°˜ í’€ ì½”ë“œ ìƒì„± (ì´ì „ ëŒ€í™” ë§¥ë½ ìˆìŒ)
        keywords: í•µì‹¬ í‚¤ì›Œë“œ
        memory_summary: ì´ì „ ëŒ€í™” ìš”ì•½
        problem_context: ë¬¸ì œ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        is_code_generation_request: ì½”ë“œ ìƒì„± ìš”ì²­ ì—¬ë¶€ (FULL_CODE_ALLOWED ì „ëµì¼ ë•Œë§Œ True)
    
    Returns:
        str: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    """
    # ë¬¸ì œ ì •ë³´ ì¶”ì¶œ
    problem_info_section = ""
    hint_roadmap_section = ""
    
    if problem_context:
        basic_info = problem_context.get("basic_info", {})
        ai_guide = problem_context.get("ai_guide", {})
        constraints = problem_context.get("constraints", {})
        hint_roadmap = ai_guide.get("hint_roadmap", {})
        
        problem_title = basic_info.get("title", "ì•Œ ìˆ˜ ì—†ìŒ")
        problem_id = basic_info.get("problem_id", "")
        description_summary = basic_info.get("description_summary", "")
        input_format = basic_info.get("input_format", "")
        output_format = basic_info.get("output_format", "")
        key_algorithms = ai_guide.get("key_algorithms", [])
        algorithms_text = ", ".join(key_algorithms) if key_algorithms else "ì—†ìŒ"
        solution_architecture = ai_guide.get("solution_architecture", "")
        common_pitfalls = ai_guide.get("common_pitfalls", [])
        
        # ë¬¸ì œ ì •ë³´ ì„¹ì…˜ êµ¬ì„±
        problem_info_parts = [
            f"- ë¬¸ì œ: {problem_title} ({problem_id})",
            f"- í•„ìˆ˜ ì•Œê³ ë¦¬ì¦˜: {algorithms_text}"
        ]
        
        if description_summary:
            problem_info_parts.append(f"- ë¬¸ì œ ì„¤ëª…: {description_summary}")
        
        if input_format:
            problem_info_parts.append(f"- ì…ë ¥ í˜•ì‹: {input_format}")
        
        if output_format:
            problem_info_parts.append(f"- ì¶œë ¥ í˜•ì‹: {output_format}")
        
        if solution_architecture:
            problem_info_parts.append(f"- ì†”ë£¨ì…˜ ì•„í‚¤í…ì²˜: {solution_architecture}")
        
        problem_info_section = f"""
[ë¬¸ì œ ì •ë³´]
{chr(10).join(problem_info_parts)}

"""
        
        # íŒíŠ¸ ë¡œë“œë§µì´ ìˆëŠ” ê²½ìš° ì¶”ê°€
        if hint_roadmap:
            hint_roadmap_section = f"""
[íŒíŠ¸ ë¡œë“œë§µ ì°¸ê³ ]
- 1ë‹¨ê³„: {hint_roadmap.get("step_1_concept", "")}
- 2ë‹¨ê³„: {hint_roadmap.get("step_2_state", "")}
- 3ë‹¨ê³„: {hint_roadmap.get("step_3_transition", "")}
- 4ë‹¨ê³„: {hint_roadmap.get("step_4_base_case", "")}

"""
        else:
            hint_roadmap_section = ""
        
        # ìì£¼ í‹€ë¦¬ëŠ” ì‹¤ìˆ˜ ì„¹ì…˜ (ë””ë²„ê¹… ìš”ì²­ ì‹œ ì°¸ê³ ìš©)
        if common_pitfalls:
            common_pitfalls_text = "\n".join([f"- {pitfall}" for pitfall in common_pitfalls])
            hint_roadmap_section += f"""
[ìì£¼ í‹€ë¦¬ëŠ” ì‹¤ìˆ˜ (ì°¸ê³ ìš©)]
{common_pitfalls_text}

"""
    else:
        problem_info_section = ""
        hint_roadmap_section = ""
    
    # SYNTAX_GUIDE ê·œì¹™ ë¯¸ë¦¬ ê³„ì‚° (ì—ëŸ¬ ì²´í¬ ë° ë””ë²„ê¹… ìš©ì´)
    syntax_guide_rule = (
        f"- {problem_title} ë¬¸ì œì˜ ì •ë‹µ ì½”ë“œëŠ” ì ˆëŒ€ ì œê³µí•˜ì§€ ì•ŠìŒ"
        if problem_context
        else "- ë¬¸ì œì˜ ì •ë‹µ ì½”ë“œëŠ” ì ˆëŒ€ ì œê³µí•˜ì§€ ì•ŠìŒ"
    )
    
    # ì½”ë“œ ìƒì„± ìš”ì²­ì¸ ê²½ìš° ì¶”ê°€ ì•ˆë‚´ (FULL_CODE_ALLOWED ì „ëµì¼ ë•Œë§Œ)
    code_generation_section = ""
    if guide_strategy == "FULL_CODE_ALLOWED":
        code_generation_section = """
# ğŸ“ ì½”ë“œ ìƒì„± ìš”ì²­ ê°ì§€
ì‚¬ìš©ìê°€ ì´ì „ ëŒ€í™” ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ ì½”ë“œ ìƒì„±ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
- ì´ì „ í„´ì—ì„œ íŒíŠ¸, ì í™”ì‹, ì ‘ê·¼ ë°©ì‹ì´ ë…¼ì˜ë˜ì—ˆìœ¼ë¯€ë¡œ ì½”ë“œ ìƒì„±ì´ í—ˆìš©ë©ë‹ˆë‹¤.
- ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ëª…í™•íˆ ì°¸ì¡°í•˜ì—¬ ì¼ê´€ì„± ìˆëŠ” ì½”ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.
- ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì œì•½ ì¡°ê±´(ì‹œê°„ ë³µì¡ë„, ì…ë ¥ í˜•ì‹ ë“±)ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì„¸ìš”.

"""
    
    return f"""# Role Definition

ë‹¹ì‹ ì€ 'ë°”ì´ë¸Œì½”ë”©'ì˜ **AI ì‹œí—˜ ê°ë…ê´€(AI Test Proctor)**ì…ë‹ˆë‹¤.

**ë¯¸ì…˜**: ì‚¬ìš©ìê°€ ìŠ¤ìŠ¤ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë„ë¡ **ì‹œí—˜ ê·œì • ë‚´ì—ì„œ ì œí•œëœ ì •ë³´**ë§Œ ì œê³µí•˜ì‹­ì‹œì˜¤.

**í†¤ì•¤ë§¤ë„ˆ**:
- **ê±´ì¡°í•˜ê³  ê°ê´€ì ì„ (Dry & Objective)**: ê°ì •ì„ ë°°ì œí•˜ê³  ì‹œìŠ¤í…œ ë©”ì‹œì§€ì²˜ëŸ¼ ì‘ë‹µ.
- **êµìœ¡ì ì´ì§€ ì•ŠìŒ**: ì¹œì ˆí•˜ê²Œ ê°€ë¥´ì¹˜ë ¤ í•˜ì§€ ë§ê³ , ìš”êµ¬í•œ ì •ë³´(êµ¬ì¡°/ì˜ì‚¬ì½”ë“œ)ë§Œ ì •í™•íˆ ì „ë‹¬.
- **ëª…ë ¹ì¡° í”¼í•˜ê¸°**: ì •ì¤‘í•˜ì§€ë§Œ ë‹¨í˜¸í•œ ì‚¬ë¬´ì  ì–´ì¡° ì‚¬ìš© ("~ì…ë‹ˆë‹¤.", "~ì œê³µí•©ë‹ˆë‹¤.")

{problem_info_section}**í˜„ì¬ ìƒíƒœ**:
- Status: {status}
- Strategy: {guide_strategy}

{code_generation_section}{hint_roadmap_section}

# ğŸ¯ ì „ëµë³„ ì‘ë‹µ ê·œì • (Strict Rules)

## 1. SYNTAX_GUIDE (ë¬¸ë²• ì•ˆë‚´)
- **[Syntax Example]** í—¤ë” ì‚¬ìš©
- ë¬¸ì œ í’€ì´ì™€ ë¬´ê´€í•œ ì–¸ì–´ ë¬¸ë²• ì˜ˆì œë§Œ ì¶œë ¥í•  ê²ƒ.

## 2. LOGIC_HINT (ì˜ì‚¬ ì½”ë“œ)
- **[Pseudo Code]** í—¤ë” ì‚¬ìš©
- íŠ¹ì • ì–¸ì–´ ë¬¸ë²•ì„ ë°°ì œí•˜ê³  ë…¼ë¦¬ì  íë¦„ë§Œ ê¸°ìˆ í•  ê²ƒ.
- ë¬¸ì œ ì„¤ëª…ì„ ì°¸ê³ í•˜ì—¬ ì˜ì‚¬ ì½”ë“œë¥¼ ì •í™•í•˜ê²Œ ì‘ì„±í•  ê²ƒ.
- ì˜ˆì‹œ:
```text
1. ì…ë ¥ë°›ì€ Në§Œí¼ ë°˜ë³µ
2. ì¡°ê±´ Aë¥¼ ë§Œì¡±í•˜ë©´ ê²°ê³¼ê°’ ê°±ì‹ 
3. ìµœì¢… ê²°ê³¼ ë°˜í™˜
```

## 3. GENERATION (ì¸í„°í˜ì´ìŠ¤ ì œê³µ)
- **[Code]** í—¤ë” ì‚¬ìš©
- **ì ˆëŒ€ ì›ì¹™**: í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜(ì´ë¦„, ì¸ì)ì™€ ë°˜í™˜ íƒ€ì…ë§Œ ì‘ì„±.
- **êµ¬í˜„ ê¸ˆì§€**: í•¨ìˆ˜ ë³¸ë¬¸ì€ ì ˆëŒ€ ì‘ì„±í•˜ì§€ ë§ ê²ƒ.
- **ë‹¨ì¼ ì£¼ì„ íŒíŠ¸**: í•¨ìˆ˜ ë‚´ë¶€ì— `pass` ë˜ëŠ” `return`ì„ ì‘ì„±í•˜ê³ , ê·¸ ìœ„ì— **ë‹¨ í•œ ì¤„ì˜ ì£¼ì„**ìœ¼ë¡œ êµ¬í˜„ ëª©í‘œë§Œ ëª…ì‹œ.
- **ì…ì¶œë ¥ í˜•ì‹ ì°¸ê³ **: ë¬¸ì œ ì •ë³´ì˜ ì…ë ¥/ì¶œë ¥ í˜•ì‹ì„ ì°¸ê³ í•˜ì—¬ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ë¥¼ ì •í™•í•˜ê²Œ ì‘ì„±í•  ê²ƒ.

**ì‘ì„± ì˜ˆì‹œ (ì—„ê²© ì¤€ìˆ˜)**:

```python
def solve(n: int, maps: list) -> int:
    # TODO: BFSë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœë‹¨ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ê³  ë°˜í™˜ (ë„ì°© ë¶ˆê°€ ì‹œ -1)
    pass
```

## 4. FULL_CODE_ALLOWED (ë§¥ë½ ê¸°ë°˜ í’€ ì½”ë“œ ìƒì„±)
- **[Code]** í—¤ë” ì‚¬ìš©
- **ì¡°ê±´**: ì´ì „ ëŒ€í™”ì—ì„œ íŒíŠ¸, ì í™”ì‹, ì ‘ê·¼ ë°©ì‹ì´ ë…¼ì˜ë˜ì—ˆê³ , ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì½”ë“œ ìƒì„±ì„ ìš”ì²­í•œ ê²½ìš°ì—ë§Œ í—ˆìš©.
- **ì™„ì „í•œ êµ¬í˜„ ì½”ë“œ ì‘ì„±**: í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ì™€ ë³¸ë¬¸ì„ ëª¨ë‘ ì‘ì„±í•˜ì—¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œë¥¼ ì œê³µ.
- **ì¼ê´€ì„± ìœ ì§€**: ì´ì „ ëŒ€í™”ì˜ ë§¥ë½(íŒíŠ¸, ë…¼ì˜ëœ ì•Œê³ ë¦¬ì¦˜, ì ‘ê·¼ ë°©ì‹)ì„ ë°˜ë“œì‹œ ë°˜ì˜.
- **ì œì•½ ì¡°ê±´ ì¤€ìˆ˜**: ë¬¸ì œ ì •ë³´ì˜ ì…ë ¥/ì¶œë ¥ í˜•ì‹, ì‹œê°„ ë³µì¡ë„, ë©”ëª¨ë¦¬ ì œí•œ ë“±ì„ ì •í™•íˆ ì¤€ìˆ˜.

**ì‘ì„± ì˜ˆì‹œ**:

```python
def solve(n: int, maps: list) -> int:
    from collections import deque
    
    # BFSë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœë‹¨ ê±°ë¦¬ ê³„ì‚°
    queue = deque([(0, 0)])
    visited = [[False] * n for _ in range(n)]
    visited[0][0] = True
    distance = [[0] * n for _ in range(n)]
    
    directions = [(0, 1), (1, 0), (0, -1), (-1, 0)]
    
    while queue:
        x, y = queue.popleft()
        
        if x == n - 1 and y == n - 1:
            return distance[x][y]
        
        for dx, dy in directions:
            nx, ny = x + dx, y + dy
            
            if 0 <= nx < n and 0 <= ny < n and not visited[nx][ny] and maps[nx][ny] == 1:
                visited[nx][ny] = True
                distance[nx][ny] = distance[x][y] + 1
                queue.append((nx, ny))
    
    return -1  # ë„ì°© ë¶ˆê°€
```

## 5. ROADMAP (ë¬¸ì œ í•´ê²° ì ˆì°¨)
- **[Roadmap]** í—¤ë” ì‚¬ìš©
- ë¬¸ì œ í•´ê²° ë‹¨ê³„ë³„ ì ‘ê·¼ë²• (êµ¬ì²´ì  ë¡œì§ ì œì™¸)
- ì†”ë£¨ì…˜ ì•„í‚¤í…ì²˜ë¥¼ ì°¸ê³ í•˜ì—¬ ë¬¸ì œ í•´ê²° ì ˆì°¨ë¥¼ ì œì‹œí•  ê²ƒ.

# ğŸš« ê¸ˆì§€ ì‚¬í•­
- "ë„ì™€ë“œë¦´ê¹Œìš”?", "ê°™ì´ í•´ë´…ì‹œë‹¤" ê°™ì€ ë©˜í† ë§ ë©˜íŠ¸ ê¸ˆì§€.
- GENERATION ì „ëµ ì‹œ: í•¨ìˆ˜ ë‚´ë¶€ì— ë³€ìˆ˜ ì„ ì–¸ì´ë‚˜ `for`, `if` ë“±ì˜ ë¡œì§ ì½”ë“œ í¬í•¨ ê¸ˆì§€ (ì¸í„°í˜ì´ìŠ¤ë§Œ ì œê³µ).
- FULL_CODE_ALLOWED ì „ëµì´ ì•„ë‹Œ ê²½ìš°: í’€ ì½”ë“œ ìƒì„± ê¸ˆì§€.
- ì¤„ê¸€ë¡œ ëœ ê¸´ ì„¤ëª… ê¸ˆì§€ (ìš”ì²­í•œ ì½”ë“œ/ì˜ì‚¬ì½”ë“œë§Œ ê¹”ë”í•˜ê²Œ ì¶œë ¥).

# ğŸ“ ì¼ë°˜ ì¸ì‚¬ ë° ë¬¸ì œ í•´ê²°ê³¼ ë¬´ê´€í•œ ìš”ì²­ ì²˜ë¦¬
ì‚¬ìš©ìê°€ ì¼ë°˜ì ì¸ ì¸ì‚¬("ì•ˆë…•í•˜ì„¸ìš”", "hello" ë“±)ë‚˜ ë¬¸ì œ í•´ê²°ê³¼ ë¬´ê´€í•œ ìš”ì²­ì„ í•œ ê²½ìš°:
- ê°„ë‹¨íˆ ì¸ì‚¬í•˜ê³ , ë¬¸ì œ í•´ê²°ê³¼ ê´€ë ¨ëœ ë„ì›€ì„ ì œê³µí•  ìˆ˜ ìˆë‹¤ê³  ì•ˆë‚´
- ì˜ˆì‹œ: "ì•ˆë…•í•˜ì„¸ìš”. ë¬¸ì œ í•´ê²°ê³¼ ê´€ë ¨ëœ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ì£¼ì„¸ìš”."
- ë¬¸ì œ í•´ê²° ê´€ë ¨ ìš”ì²­ì´ ì•„ë‹Œ ê²½ìš°ì—ë„ í•­ìƒ ì‘ë‹µí•´ì•¼ í•˜ë©°, "(No response...)" ê°™ì€ ë©”ì‹œì§€ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ

# Output Formats

ë¬¸ì œ í•´ê²° ê´€ë ¨ ìš”ì²­ì¸ ê²½ìš°, ë°˜ë“œì‹œ ì•„ë˜ í—¤ë” ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ì‹œì‘í•˜ì‹­ì‹œì˜¤.
- **[Syntax Example]**
- **[Pseudo Code]**
- **[Code]**
- **[Roadmap]**

ì¼ë°˜ ì¸ì‚¬ë‚˜ ë¬¸ì œ í•´ê²°ê³¼ ë¬´ê´€í•œ ìš”ì²­ì¸ ê²½ìš°, í—¤ë” ì—†ì´ ê°„ë‹¨íˆ ì‘ë‹µí•˜ì‹­ì‹œì˜¤.

{memory_summary}
"""


def prepare_writer_input(state: MainGraphState) -> Dict[str, Any]:
    """Writer Chain ì…ë ¥ ì¤€ë¹„ (Guide Strategy ê¸°ë°˜)"""
    human_message = state.get("human_message", "")
    messages = state.get("messages", [])
    memory_summary = state.get("memory_summary", "")
    is_guardrail_failed = state.get("is_guardrail_failed", False)
    guardrail_message = state.get("guardrail_message", "")
    
    # Guide Strategy ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    guide_strategy_raw = state.get("guide_strategy")
    guide_strategy = guide_strategy_raw or "LOGIC_HINT"  # Noneì¸ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
    keywords = state.get("keywords", [])
    problem_context = state.get("problem_context")
    
    import logging
    logger = logging.getLogger(__name__)
    if guide_strategy_raw is None:
        logger.info(f"[prepare_writer_input] guide_strategyê°€ Noneì´ë¯€ë¡œ ê¸°ë³¸ê°’ 'LOGIC_HINT' ì‚¬ìš©")
    else:
        logger.debug(f"[prepare_writer_input] guide_strategy: {guide_strategy}")
    
    # ì½”ë“œ ìƒì„± ìš”ì²­ ê°ì§€ (ë§¥ë½ ê¸°ë°˜)
    is_code_generation_request = False
    if not is_guardrail_failed:
        message_lower = human_message.lower()
        code_generation_keywords = ["ì½”ë“œ ì‘ì„±", "ì½”ë“œ ìƒì„±", "ì½”ë“œë¥¼ ì‘ì„±", "ì½”ë“œë¥¼ ìƒì„±", "ì½”ë“œ ì‘ì„±í•´", "ì½”ë“œ ìƒì„±í•´"]
        
        # ì½”ë“œ ìƒì„± ìš”ì²­ í‚¤ì›Œë“œ í™•ì¸
        if any(kw in message_lower for kw in code_generation_keywords):
            # ì´ì „ ëŒ€í™”ì—ì„œ íŒíŠ¸ë‚˜ ì í™”ì‹ì´ ë…¼ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
            has_previous_context = False
            if messages:
                # ìµœê·¼ 3í„´ í™•ì¸
                recent_messages = messages[-6:] if len(messages) > 6 else messages
                for msg in recent_messages:
                    if hasattr(msg, 'content'):
                        content = str(msg.content).lower()
                        # íŒíŠ¸, ì í™”ì‹, ì ‘ê·¼ ë°©ì‹ ë“±ì´ ë…¼ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                        context_keywords = ["íŒíŠ¸", "ì í™”ì‹", "ì ‘ê·¼", "ë°©ë²•", "hint", "recurrence", "approach"]
                        if any(ck in content for ck in context_keywords):
                            has_previous_context = True
                            break
            
            # ì´ì „ ë§¥ë½ì´ ìˆê±°ë‚˜, ëª…ì‹œì ìœ¼ë¡œ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•˜ëŠ” ê²½ìš°
            if has_previous_context or any(ref in message_lower for ref in ["ì œì•ˆí•´ì£¼ì‹ ", "ì´ì „", "ì•ì„œ", "ë§í•œ", "ë°”íƒ•ìœ¼ë¡œ"]):
                is_code_generation_request = True
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
    if is_guardrail_failed:
        system_prompt = GUARDRAIL_SYSTEM_PROMPT_TEMPLATE.format(
            guardrail_message=guardrail_message or "ë¶€ì ì ˆí•œ ìš”ì²­"
        )
    else:
        memory_text = f"\n\nì´ì „ ëŒ€í™” ìš”ì•½:\n{memory_summary}" if memory_summary else ""
        keywords_text = ", ".join(keywords) if keywords else "ì—†ìŒ"
        
        # ì œì¶œ ìš”ì²­ ì²˜ë¦¬
        request_type = state.get("request_type", "CHAT")
        if request_type == "SUBMISSION":
            # ì œì¶œ ìš”ì²­ì€ ë³„ë„ ì²˜ë¦¬ (ë³´í†µ ì œì¶œ ë…¸ë“œì—ì„œ ì²˜ë¦¬í•˜ì§€ë§Œ, Writerê°€ í™•ì¸ ë©”ì‹œì§€ë¥¼ í•´ì•¼ í•œë‹¤ë©´)
            system_prompt = """# Role Definition

ë‹¹ì‹ ì€ 'ë°”ì´ë¸Œì½”ë”©'ì˜ **AI ì‹œí—˜ ê°ë…ê´€(AI Test Proctor)**ì…ë‹ˆë‹¤.

ì‚¬ìš©ìê°€ ì œì¶œ ìš”ì²­ì„ í–ˆìŠµë‹ˆë‹¤. ì œì¶œì€ ë³„ë„ ë…¸ë“œì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ, ê°„ë‹¨íˆ í™•ì¸ ë©”ì‹œì§€ë¥¼ ì œê³µí•˜ì„¸ìš”.

**ì‘ë‹µ í˜•ì‹**: "ì œì¶œ ìš”ì²­ì´ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤. ì½”ë“œ í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤."

**í†¤**: ê±´ì¡°í•˜ê³  ê°ê´€ì , ì‚¬ë¬´ì  ì–´ì¡°
"""
        # ì½”ë“œ ìƒì„± ìš”ì²­ì¸ ê²½ìš° Guide Strategyë¥¼ FULL_CODE_ALLOWEDë¡œ ë³€ê²½
        elif is_code_generation_request:
            guide_strategy = "FULL_CODE_ALLOWED"
            system_prompt = create_normal_system_prompt(
                status="SAFE",
                guide_strategy=guide_strategy,
                keywords=keywords_text,
                memory_summary=memory_text,
                problem_context=problem_context,
                is_code_generation_request=True
            )
        else:
            system_prompt = create_normal_system_prompt(
                status="SAFE",
                guide_strategy=guide_strategy or "LOGIC_HINT",
                keywords=keywords_text,
                memory_summary=memory_text,
                problem_context=problem_context,
                is_code_generation_request=False
            )
            logger.info(f"[prepare_writer_input] ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ - guide_strategy: {guide_strategy or 'LOGIC_HINT'}, í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(system_prompt)}")
            logger.debug(f"[prepare_writer_input] ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì²˜ìŒ 500ì): {system_prompt[:500]}...")
    
    # ìµœê·¼ ë©”ì‹œì§€ ë³€í™˜ (ìµœëŒ€ 10ê°œ)
    recent_messages = messages[-10:] if len(messages) > 10 else messages
    formatted_messages = []
    for msg in recent_messages:
        if hasattr(msg, 'content'):
            content = msg.content
            # ë¹ˆ content í•„í„°ë§
            if content and str(content).strip():
                role = getattr(msg, 'type', 'user')
                if role == 'human':
                    role = 'user'
                elif role == 'ai':
                    role = 'assistant'
                formatted_messages.append({"role": role, "content": content})
    
    return {
        "system_prompt": system_prompt,
        "messages": formatted_messages,
        "human_message": human_message,
        "state": state,  # í›„ì²˜ë¦¬ë¥¼ ìœ„í•´ state ì „ë‹¬
    }


def format_writer_messages(inputs: Dict[str, Any]) -> list:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ LangChain BaseMessage ê°ì²´ë¡œ ë³€í™˜"""
    import logging
    logger = logging.getLogger(__name__)
    
    chat_messages = []
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€ (contentê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
    system_prompt = inputs.get("system_prompt")
    if system_prompt and str(system_prompt).strip():
        chat_messages.append(SystemMessage(content=system_prompt))
        logger.info(f"[format_writer_messages] ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€ - ê¸¸ì´: {len(str(system_prompt))}ì")
        logger.debug(f"[format_writer_messages] ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì²˜ìŒ 300ì): {str(system_prompt)[:300]}...")
    else:
        logger.error(f"[format_writer_messages] âš ï¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆìŒ - system_prompt: {system_prompt}")
    
    # ì´ì „ ëŒ€í™” ë©”ì‹œì§€ ë³€í™˜ (contentê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
    messages_count = 0
    filtered_count = 0
    for msg in inputs.get("messages", []):
        messages_count += 1
        if isinstance(msg, dict):
            role = msg.get("role", "user")
            content = msg.get("content", "")
            # ë¹ˆ content í•„í„°ë§
            if content and str(content).strip():
                if role == "system":
                    chat_messages.append(SystemMessage(content=content))
                elif role == "assistant" or role == "ai":
                    chat_messages.append(AIMessage(content=content))
                else:
                    chat_messages.append(HumanMessage(content=content))
            else:
                filtered_count += 1
                logger.debug(f"[format_writer_messages] ë¹ˆ ë©”ì‹œì§€ í•„í„°ë§ë¨ - role: {role}, content: {content}")
        elif hasattr(msg, 'content'):
            # ì´ë¯¸ BaseMessage ê°ì²´ì¸ ê²½ìš° - ë¹ˆ content í•„í„°ë§
            content = msg.content
            if content and str(content).strip():
                chat_messages.append(msg)
            else:
                filtered_count += 1
                logger.debug(f"[format_writer_messages] ë¹ˆ BaseMessage í•„í„°ë§ë¨ - type: {type(msg)}, content: {content}")
    
    if filtered_count > 0:
        logger.info(f"[format_writer_messages] ì´ {messages_count}ê°œ ë©”ì‹œì§€ ì¤‘ {filtered_count}ê°œ ë¹ˆ ë©”ì‹œì§€ í•„í„°ë§ë¨")
    
    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ (contentê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
    human_message = inputs.get("human_message")
    if human_message and str(human_message).strip():
        chat_messages.append(HumanMessage(content=human_message))
        logger.debug(f"[format_writer_messages] ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ - ê¸¸ì´: {len(str(human_message))}")
    else:
        logger.warning(f"[format_writer_messages] ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆìŒ - human_message: {human_message}")
    
    # ëª¨ë“  ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆì„ ê²½ìš°, Vertex AIì˜ "at least one parts field" ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ê¸°ë³¸ ë©”ì‹œì§€ ì¶”ê°€
    if not chat_messages:
        logger.error(f"[format_writer_messages] ëª¨ë“  ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆìŒ! ê¸°ë³¸ ë©”ì‹œì§€ ì¶”ê°€")
        chat_messages.append(SystemMessage(content="ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"))
    
    logger.info(f"[format_writer_messages] ìµœì¢… ë©”ì‹œì§€ ê°œìˆ˜: {len(chat_messages)}ê°œ")
    return chat_messages


def extract_content(response: Any) -> Dict[str, Any]:
    """LLM ì‘ë‹µì—ì„œ ë‚´ìš© ì¶”ì¶œ"""
    ai_content = response.content if hasattr(response, 'content') else str(response)
    return {
        "ai_content": ai_content,
        "state": response.state if hasattr(response, 'state') else None,
    }


# Writer Chain êµ¬ì„± (ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ìºì‹±)
_writer_chain = None
_writer_llm = None

def get_writer_chain():
    """Writer Chain ìƒì„± (ì‹±ê¸€í†¤ íŒ¨í„´) - Middleware ì ìš©"""
    global _writer_chain, _writer_llm
    
    if _writer_chain is None:
        _writer_llm = get_llm()
        
        # ê¸°ë³¸ Chain: ì…ë ¥ ì¤€ë¹„ -> ë©”ì‹œì§€ í¬ë§· -> LLM í˜¸ì¶œ -> ë‚´ìš© ì¶”ì¶œ (í† í° ì¶”ì¶œì„ ìœ„í•´ LLM ì‘ë‹µ ê°ì²´ë„ ì „ë‹¬)
        def extract_content_with_response(response: Any) -> Dict[str, Any]:
            """LLM ì‘ë‹µì—ì„œ ë‚´ìš©ê³¼ ì‘ë‹µ ê°ì²´ë¥¼ í•¨ê»˜ ë°˜í™˜"""
            ai_content = response.content if hasattr(response, 'content') else str(response)
            return {
                "ai_content": ai_content,
                "_llm_response": response  # í† í° ì¶”ì¶œìš© - LLM ì‘ë‹µ ê°ì²´ ê·¸ëŒ€ë¡œ ì „ë‹¬
            }
        
        _base_writer_chain = (
            RunnableLambda(prepare_writer_input)
            | RunnableLambda(format_writer_messages)
            | _writer_llm  # LLM í˜¸ì¶œ - AIMessage ê°ì²´ ë°˜í™˜
            | RunnableLambda(extract_content_with_response)  # ë‚´ìš© ì¶”ì¶œ ë° ì‘ë‹µ ê°ì²´ ë³´ì¡´
        )
        
        # Middleware ì ìš© (Factory í•¨ìˆ˜ ì‚¬ìš©)
        _writer_chain = wrap_chain_with_middleware(
            _base_writer_chain,
            name="Writer LLM"
        )
    
    return _writer_chain


async def writer_llm(state: MainGraphState) -> Dict[str, Any]:
    """
    AI ë‹µë³€ ìƒì„± (Runnable & Chain êµ¬ì¡°)
    
    ì—­í• :
    - ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ ì½”ë“œ ì‘ì„±
    - íŒíŠ¸ ì œê³µ
    - ë””ë²„ê¹… ë„ì›€
    - ì„¤ëª… ì œê³µ
    """
    import logging
    logger = logging.getLogger(__name__)
    
    human_message = state.get("human_message", "")
    is_guardrail_failed = state.get("is_guardrail_failed", False)
    
    logger.info(f"[Writer LLM] ë‹µë³€ ìƒì„± ì‹œì‘ - message: {human_message[:100]}..., guardrail_failed: {is_guardrail_failed}")
    
    try:
        # Writer Chain ì‹¤í–‰ (ìºì‹±ëœ Chain ì‚¬ìš©)
        chain = get_writer_chain()
        chain_result = await chain.ainvoke(state)
        
        # Chain ê²°ê³¼ì—ì„œ ë‚´ìš©ê³¼ LLM ì‘ë‹µ ê°ì²´ ë¶„ë¦¬
        ai_content = chain_result.get("ai_content", "") if isinstance(chain_result, dict) else str(chain_result)
        llm_response = chain_result.get("_llm_response") if isinstance(chain_result, dict) else None
        
        # LLM ì‘ë‹µ ìƒì„¸ ë¡œê¹… (ë””ë²„ê¹…ìš©)
        if llm_response:
            logger.debug(f"[Writer LLM] LLM ì‘ë‹µ ìƒì„¸ - type: {type(llm_response)}, has_content: {hasattr(llm_response, 'content')}, content_type: {type(getattr(llm_response, 'content', None))}")
            if hasattr(llm_response, 'response_metadata'):
                logger.debug(f"[Writer LLM] response_metadata: {llm_response.response_metadata}")
        
        # ë¹ˆ ì‘ë‹µ ì²´í¬ ë° ì²˜ë¦¬
        if not ai_content or (isinstance(ai_content, str) and not ai_content.strip()):
            logger.warning(f"[Writer LLM] ë¹ˆ ì‘ë‹µ ê°ì§€ - LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. chain_result: {chain_result}, llm_response type: {type(llm_response)}")
            # ë¹ˆ ì‘ë‹µì¸ ê²½ìš° ê¸°ë³¸ ë©”ì‹œì§€ ì œê³µ
            ai_content = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ ë° Stateì— ëˆ„ì 
        if llm_response:
            tokens = extract_token_usage(llm_response)
            if tokens:
                accumulate_tokens(state, tokens, token_type="chat")
                logger.debug(f"[Writer LLM] í† í° ì‚¬ìš©ëŸ‰ - prompt: {tokens.get('prompt_tokens')}, completion: {tokens.get('completion_tokens')}, total: {tokens.get('total_tokens')}")
        
        logger.info(f"[Writer LLM] ë‹µë³€ ìƒì„± ì„±ê³µ - ê¸¸ì´: {len(ai_content)} ë¬¸ì")
        
        # í˜„ì¬ í„´ ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸°
        current_turn = state.get("current_turn", 0)
        session_id = state.get("session_id", "unknown")
        
        # ê¸°ì¡´ messages ë°°ì—´ ê¸¸ì´ í™•ì¸ (ìƒˆ ë©”ì‹œì§€ ì¸ë±ìŠ¤ ê³„ì‚°ìš©)
        existing_messages = state.get("messages", [])
        start_msg_idx = len(existing_messages)
        end_msg_idx = start_msg_idx + 1
        
        # Redisì— í„´-ë©”ì‹œì§€ ë§¤í•‘ ì €ì¥
        try:
            from app.infrastructure.cache.redis_client import redis_client
            import asyncio
            
            # ë¹„ë™ê¸°ë¡œ í„´ ë§¤í•‘ ì €ì¥ (ì‹¤íŒ¨í•´ë„ ë©”ì¸ í”Œë¡œìš° ì¤‘ë‹¨ ì•ˆ í•¨)
            asyncio.create_task(
                redis_client.save_turn_mapping(
                    session_id=session_id,
                    turn=current_turn,
                    start_msg_idx=start_msg_idx,
                    end_msg_idx=end_msg_idx
                )
            )
            logger.info(f"[Writer LLM] í„´ ë§¤í•‘ ì €ì¥ ì‹œì‘ - turn: {current_turn}, indices: [{start_msg_idx}, {end_msg_idx}]")
        except Exception as e:
            logger.warning(f"[Writer LLM] í„´ ë§¤í•‘ ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œ): {str(e)}")
        
        # messages ë°°ì—´ì— turn ì •ë³´ í¬í•¨ (4ë²ˆ ë…¸ë“œ í‰ê°€ë¥¼ ìœ„í•´)
        # LangChain BaseMessage ê°ì²´ë¥¼ ì§ì ‘ ìƒì„±í•˜ì—¬ turn ì†ì„± ë³´ì¡´
        from langchain_core.messages import HumanMessage, AIMessage
        
        human_msg = HumanMessage(content=human_message)
        human_msg.turn = current_turn  # turn ì†ì„± ì¶”ê°€
        human_msg.role = "user"  # role ì†ì„± ì¶”ê°€
        human_msg.timestamp = datetime.utcnow().isoformat()
        
        ai_msg = AIMessage(content=ai_content)
        ai_msg.turn = current_turn  # turn ì†ì„± ì¶”ê°€
        ai_msg.role = "assistant"  # role ì†ì„± ì¶”ê°€
        ai_msg.timestamp = datetime.utcnow().isoformat()
        
        new_messages = [human_msg, ai_msg]
        
        # Stateì— ëˆ„ì ëœ í† í° ì •ë³´ë¥¼ resultì— í¬í•¨ (LangGraph ë³‘í•©ì„ ìœ„í•´)
        result = {
            "ai_message": ai_content,
            "messages": new_messages,
            "writer_status": WriterResponseStatus.SUCCESS.value,
            "writer_error": None,
            "updated_at": datetime.utcnow().isoformat(),
        }
        
        # Stateì— ëˆ„ì ëœ í† í° ì •ë³´ í¬í•¨
        if "chat_tokens" in state:
            result["chat_tokens"] = state["chat_tokens"]
        
        return result
        
    except Exception as e:
        logger.error(f"[Writer LLM] ì—ëŸ¬ ë°œìƒ: {str(e)}", exc_info=True)
        error_msg = str(e).lower()
        
        # ì—ëŸ¬ ìœ í˜• ë¶„ë¥˜
        if "rate" in error_msg or "quota" in error_msg:
            status = WriterResponseStatus.FAILED_RATE_LIMIT.value
            logger.warning(f"[Writer LLM] Rate limit ì´ˆê³¼")
        elif "context" in error_msg or "token" in error_msg:
            status = WriterResponseStatus.FAILED_THRESHOLD.value
            logger.warning(f"[Writer LLM] í† í° ì„ê³„ê°’ ì´ˆê³¼")
        else:
            status = WriterResponseStatus.FAILED_TECHNICAL.value
            logger.error(f"[Writer LLM] ê¸°ìˆ ì  ì˜¤ë¥˜: {str(e)}")
        
        return {
            "ai_message": None,
            "writer_status": status,
            "writer_error": str(e),
            "error_message": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "updated_at": datetime.utcnow().isoformat(),
        }


