"""
ë…¸ë“œ 3: Writer LLM
AI ë‹µë³€ ìƒì„± (Runnable & Chain êµ¬ì¡°)
"""

from typing import Dict, Any, Optional
from datetime import datetime

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_vertexai import ChatVertexAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from app.domain.langgraph.states import MainGraphState
from app.core.config import settings
from app.infrastructure.persistence.models.enums import WriterResponseStatus
from app.domain.langgraph.middleware import wrap_chain_with_middleware
from app.domain.langgraph.utils.token_tracking import extract_token_usage, accumulate_tokens


def get_llm():
    """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (Vertex AI ë˜ëŠ” AI Studio)"""
    if settings.USE_VERTEX_AI:
        # Vertex AI ì‚¬ìš© (GCP í¬ë ˆë”§ ì‚¬ìš©)
        import json
        from google.oauth2 import service_account
        
        credentials = None
        if settings.GOOGLE_SERVICE_ACCOUNT_JSON:
            service_account_info = json.loads(settings.GOOGLE_SERVICE_ACCOUNT_JSON)
            credentials = service_account.Credentials.from_service_account_info(
                service_account_info
            )
        
        return ChatVertexAI(
            model=settings.DEFAULT_LLM_MODEL,
            project=settings.GOOGLE_PROJECT_ID,
            location=settings.GOOGLE_LOCATION,
            credentials=credentials,
            temperature=settings.LLM_TEMPERATURE,
            max_output_tokens=settings.LLM_MAX_TOKENS,
        )
    else:
        # AI Studio ì‚¬ìš© (API Key ë°©ì‹, Free Tier)
        return ChatGoogleGenerativeAI(
            model=settings.DEFAULT_LLM_MODEL,
            google_api_key=settings.GEMINI_API_KEY,
            temperature=settings.LLM_TEMPERATURE,
            max_output_tokens=settings.LLM_MAX_TOKENS,
        )


# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
GUARDRAIL_SYSTEM_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ AI ì½”ë”© í…ŒìŠ¤íŠ¸ì˜ ë³´ì•ˆ ê´€ë¦¬ì(Gatekeeper)ì…ë‹ˆë‹¤.

# ğŸ›¡ï¸ ìƒí™©
ì‚¬ìš©ìì˜ ìš”ì²­ì´ í…ŒìŠ¤íŠ¸ ì •ì±…ì— ìœ„ë°˜ë˜ì—ˆìŠµë‹ˆë‹¤.
ìœ„ë°˜ ì´ìœ : {guardrail_message}

# âœ‹ ê±°ì ˆ ë©”ì‹œì§€ ìƒì„± ê·œì¹™
1. **ì •ì¤‘í•˜ê²Œ ê±°ì ˆ**: "í•´ë‹¹ ìš”ì²­ì€ í…ŒìŠ¤íŠ¸ ì •ì±…ìƒ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
2. **ì´ìœ  ê°„ë‹¨ ì„¤ëª…**: ì™œ ê±°ì ˆí•˜ëŠ”ì§€ 1-2ì¤„ë¡œ ì„¤ëª…
3. **ëŒ€ì•ˆ ì œì‹œ**: ëŒ€ì‹  **ê°œë…(Concept)** ìˆ˜ì¤€ì—ì„œ í•™ìŠµ ë°©í–¥ ì œì‹œ
4. **ì†Œí¬ë¼í…ŒìŠ¤ì‹ ë°˜ë¬¸**: ì§ˆë¬¸ì„ ë˜ì ¸ ìŠ¤ìŠ¤ë¡œ ìƒê°í•˜ê²Œ ìœ ë„

# ğŸ“œ ì‘ë‹µ í˜•ì‹ ì˜ˆì‹œ
```
ì£„ì†¡í•©ë‹ˆë‹¤ë§Œ, í•´ë‹¹ ìš”ì²­ì€ ë¬¸ì œì˜ ì •ë‹µê³¼ ì§ê²°ë˜ì–´ ìˆì–´ ì§ì ‘ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.

ëŒ€ì‹ , ë‹¤ìŒ ê°œë…ë“¤ì„ ê³µë¶€í•´ë³´ì‹œëŠ” ê±´ ì–´ë–¨ê¹Œìš”?
- ë¹„íŠ¸ë§ˆìŠ¤í‚¹ìœ¼ë¡œ ìƒíƒœ í‘œí˜„í•˜ê¸°
- ë™ì  ê³„íšë²•ì˜ ë©”ëª¨ì´ì œì´ì…˜

ìŠ¤ìŠ¤ë¡œ ìƒê°í•´ë³´ì„¸ìš”: "ëª¨ë“  ë„ì‹œë¥¼ ë°©ë¬¸í–ˆëŠ”ì§€ ì–´ë–»ê²Œ í™•ì¸í•  ìˆ˜ ìˆì„ê¹Œìš”?"
```

**í†¤**: ì—„ê²©í•˜ì§€ë§Œ êµìœ¡ì , ê²©ë ¤í•˜ëŠ” íƒœë„
"""

def create_normal_system_prompt(
    status: str,
    guide_strategy: str,
    keywords: str,
    memory_summary: str,
    problem_context: Optional[Dict[str, Any]] = None,
    is_code_generation_request: bool = False
) -> str:
    """
    Writer LLM ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ë¬¸ì œ ì •ë³´ í¬í•¨)
    
    Args:
        status: ì•ˆì „ ìƒíƒœ (SAFE)
        guide_strategy: ê°€ì´ë“œ ì „ëµ (SYNTAX_GUIDE | LOGIC_HINT | ROADMAP)
        keywords: í•µì‹¬ í‚¤ì›Œë“œ
        memory_summary: ì´ì „ ëŒ€í™” ìš”ì•½
        problem_context: ë¬¸ì œ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        str: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    """
    # ë¬¸ì œ ì •ë³´ ì¶”ì¶œ
    problem_info_section = ""
    hint_roadmap_section = ""
    
    if problem_context:
        basic_info = problem_context.get("basic_info", {})
        ai_guide = problem_context.get("ai_guide", {})
        hint_roadmap = ai_guide.get("hint_roadmap", {})
        
        problem_title = basic_info.get("title", "ì•Œ ìˆ˜ ì—†ìŒ")
        problem_id = basic_info.get("problem_id", "")
        key_algorithms = ai_guide.get("key_algorithms", [])
        algorithms_text = ", ".join(key_algorithms) if key_algorithms else "ì—†ìŒ"
        
        problem_info_section = f"""
[ë¬¸ì œ ì •ë³´]
- ë¬¸ì œ: {problem_title} ({problem_id})
- í•„ìˆ˜ ì•Œê³ ë¦¬ì¦˜: {algorithms_text}

"""
        
        # íŒíŠ¸ ë¡œë“œë§µì´ ìˆëŠ” ê²½ìš° ì¶”ê°€
        if hint_roadmap:
            hint_roadmap_section = f"""
[íŒíŠ¸ ë¡œë“œë§µ ì°¸ê³ ]
- 1ë‹¨ê³„: {hint_roadmap.get("step_1_concept", "")}
- 2ë‹¨ê³„: {hint_roadmap.get("step_2_state", "")}
- 3ë‹¨ê³„: {hint_roadmap.get("step_3_transition", "")}
- 4ë‹¨ê³„: {hint_roadmap.get("step_4_base_case", "")}

"""
    else:
        problem_info_section = ""
        hint_roadmap_section = ""
    
    # SYNTAX_GUIDE ê·œì¹™ ë¯¸ë¦¬ ê³„ì‚° (ì—ëŸ¬ ì²´í¬ ë° ë””ë²„ê¹… ìš©ì´)
    syntax_guide_rule = (
        f"- {problem_title} ë¬¸ì œì˜ ì •ë‹µ ì½”ë“œëŠ” ì ˆëŒ€ ì œê³µí•˜ì§€ ì•ŠìŒ"
        if problem_context
        else "- ë¬¸ì œì˜ ì •ë‹µ ì½”ë“œëŠ” ì ˆëŒ€ ì œê³µí•˜ì§€ ì•ŠìŒ"
    )
    
    # ì½”ë“œ ìƒì„± ìš”ì²­ì¸ ê²½ìš° ì¶”ê°€ ì•ˆë‚´
    code_generation_section = ""
    if is_code_generation_request:
        code_generation_section = """
# ğŸ“ ì½”ë“œ ìƒì„± ìš”ì²­ ê°ì§€
ì‚¬ìš©ìê°€ ì´ì „ ëŒ€í™” ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ ì½”ë“œ ìƒì„±ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.
- ì´ì „ í„´ì—ì„œ íŒíŠ¸, ì í™”ì‹, ì ‘ê·¼ ë°©ì‹ì´ ë…¼ì˜ë˜ì—ˆìœ¼ë¯€ë¡œ ì½”ë“œ ìƒì„±ì´ í—ˆìš©ë©ë‹ˆë‹¤.
- ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ëª…í™•íˆ ì°¸ì¡°í•˜ì—¬ ì¼ê´€ì„± ìˆëŠ” ì½”ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.
- ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì œì•½ ì¡°ê±´(ì‹œê°„ ë³µì¡ë„, ì…ë ¥ í˜•ì‹ ë“±)ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì„¸ìš”.

"""
    
    return f"""# Role Definition

ë„ˆëŠ” ì†Œí¬ë¼í…ŒìŠ¤ì‹ êµìœ¡ë²•ì„ ì§€í–¥í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ íŠœí„° 'ë°”ì´ë¸Œì½”ë”©'ì´ë‹¤.

{problem_info_section}Node 2ì˜ ë¶„ì„ ê²°ê³¼:
- Status: {status} (SAFE)
- Guide Strategy: {guide_strategy} (SYNTAX_GUIDE | LOGIC_HINT | ROADMAP | GENERATION)
- Keywords: {keywords}
{code_generation_section}{hint_roadmap_section}

# ğŸ¯ Guide Strategyë³„ ë‹µë³€ ê·œì¹™

## SYNTAX_GUIDEì¸ ê²½ìš°:
- **[Syntax Example]** í˜•ì‹ í•„ìˆ˜
- ë¬¸ì œì™€ ë¬´ê´€í•œ ìˆœìˆ˜ ë¬¸ë²• ì˜ˆì‹œë§Œ ì œê³µ
{syntax_guide_rule}

ì˜ˆì‹œ:
```
[Syntax Example]
ë¹„íŠ¸ë§ˆìŠ¤í‚¹ì˜ ê¸°ë³¸ ë¬¸ë²• ì˜ˆì‹œ (ë¬¸ì œì™€ ë¬´ê´€):

```python
# ë¹„íŠ¸ ì‹œí”„íŠ¸ ì—°ì‚° ì˜ˆì‹œ
a = 1
print(a << 3)  # 2^3 = 8 ì¶œë ¥

# ë¹„íŠ¸ OR ì—°ì‚° ì˜ˆì‹œ
visited = 0
visited |= (1 << 3)  # 3ë²ˆ ë°©ë¬¸ í‘œì‹œ
```
```

## LOGIC_HINTì¸ ê²½ìš°:
- **[Concept]** í˜•ì‹ í•„ìˆ˜
- ì¼ë°˜ì ì¸ ì•Œê³ ë¦¬ì¦˜ ê°œë… ì„¤ëª…
- **íŒíŠ¸ ìš”ì²­ ì‹œ**: êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ íŒíŠ¸ ì œê³µ (íšŒí”¼ì ì´ì§€ ì•Šê²Œ)
- **ì í™”ì‹ íŒíŠ¸ ìš”ì²­ ì‹œ**: ì í™”ì‹ì˜ êµ¬ì¡°ì™€ ì ‘ê·¼ ë°©ì‹ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´
- ë¬¸ì œ íŠ¹ì • ì™„ì „í•œ ì •ë‹µ ì½”ë“œëŠ” ì œì™¸í•˜ë˜, íŒíŠ¸ëŠ” ì¶©ë¶„íˆ ì œê³µ

ì˜ˆì‹œ (ì¼ë°˜ ê°œë… ì§ˆë¬¸):
```
[Concept]
ë™ì  ê³„íšë²•ì€ í° ë¬¸ì œë¥¼ ì‘ì€ í•˜ìœ„ ë¬¸ì œë¡œ ë‚˜ëˆ„ì–´ í•´ê²°í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.
- ë©”ëª¨ì´ì œì´ì…˜: ê³„ì‚° ê²°ê³¼ë¥¼ ì €ì¥í•˜ì—¬ ì¤‘ë³µ ê³„ì‚° ë°©ì§€
- ì í™”ì‹: í•˜ìœ„ ë¬¸ì œ ê°„ì˜ ê´€ê³„ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„

[Question]
ìŠ¤ìŠ¤ë¡œ ìƒê°í•´ë³´ì„¸ìš”: "ì´ ë¬¸ì œì—ì„œ ì–´ë–¤ í•˜ìœ„ ë¬¸ì œë“¤ì´ ìˆì„ê¹Œìš”?"
```

ì˜ˆì‹œ (ì í™”ì‹ íŒíŠ¸ ìš”ì²­):
```
[Concept]
`dp[current_city][visited_bitmask]` ìƒíƒœì—ì„œ ì í™”ì‹ì„ ìˆ˜ë¦½í•  ë•Œ:

1. **í˜„ì¬ ìƒíƒœ**: `current_city`ì— ìˆê³ , `visited_bitmask`ì— í•´ë‹¹í•˜ëŠ” ë„ì‹œë“¤ì„ ë°©ë¬¸í•œ ìƒíƒœ
2. **ë‹¤ìŒ ë‹¨ê³„**: ì•„ì§ ë°©ë¬¸í•˜ì§€ ì•Šì€ ë„ì‹œ `next_city`ë¡œ ì´ë™
3. **ì í™”ì‹ êµ¬ì¡°**: 
   - `dp[current][visited] = min(ëª¨ë“  next_cityì— ëŒ€í•´, cost(current, next) + dp[next][visited | (1<<next)])`
   - í˜„ì¬ ë„ì‹œì—ì„œ ë‹¤ìŒ ë„ì‹œë¡œ ì´ë™í•˜ëŠ” ë¹„ìš© + ë‹¤ìŒ ë„ì‹œì—ì„œ ë‚˜ë¨¸ì§€ë¥¼ ë°©ë¬¸í•˜ëŠ” ìµœì†Œ ë¹„ìš©

[Question]
ì´ì œ ê¸°ì € ì¡°ê±´(base case)ì„ ìƒê°í•´ë³´ì„¸ìš”: ëª¨ë“  ë„ì‹œë¥¼ ë°©ë¬¸í•œ ê²½ìš°ëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬í•´ì•¼ í• ê¹Œìš”?
```
```

## ROADMAPì¸ ê²½ìš°:
- **[Roadmap]** í˜•ì‹ í•„ìˆ˜
- ë¬¸ì œ í•´ê²° ë‹¨ê³„ë³„ ì ‘ê·¼ë²•
- êµ¬ì²´ì  ë¡œì§ì€ ì œì™¸

ì˜ˆì‹œ:
```
[Roadmap]
ë¬¸ì œ í•´ê²° ë‹¨ê³„ë³„ ì ‘ê·¼ë²• (êµ¬ì²´ì  ë¡œì§ ì œì™¸):

1. ë¬¸ì œ ì´í•´: ì…ë ¥/ì¶œë ¥ í˜•ì‹ íŒŒì•…
2. ì ‘ê·¼ ë°©ë²• ì„ íƒ: ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ì‚¬ìš©í• ì§€
3. ìƒíƒœ ì •ì˜: ë™ì  ê³„íšë²•ì´ë¼ë©´ ì–´ë–¤ ìƒíƒœë¥¼ ì €ì¥í• ì§€
4. ì í™”ì‹ ì„¤ê³„: ìƒíƒœ ê°„ì˜ ê´€ê³„ ì •ì˜
5. êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸

[Question]
ìŠ¤ìŠ¤ë¡œ ìƒê°í•´ë³´ì„¸ìš”: "ê° ë‹¨ê³„ì—ì„œ ì–´ë–¤ ì •ë³´ê°€ í•„ìš”í• ê¹Œìš”?"
```
```

## GENERATIONì¸ ê²½ìš° (ì½”ë“œ ìƒì„± ìš”ì²­):
- **[Code]** í˜•ì‹ í•„ìˆ˜
- ì´ì „ ëŒ€í™” ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ ì½”ë“œ ìƒì„±
- ì´ì „ í„´ì—ì„œ ë…¼ì˜ëœ íŒíŠ¸, ì í™”ì‹, ì ‘ê·¼ ë°©ì‹ì„ ë°˜ì˜
- ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì œì•½ ì¡°ê±´ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜
- ì½”ë“œì— ì£¼ì„ì„ ì¶”ê°€í•˜ì—¬ ì´í•´ë¥¼ ë•ê¸°

ì˜ˆì‹œ:
```
[Code]
ì´ì „ì— ë…¼ì˜í•œ ì í™”ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì½”ë“œë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤:

```python
# ì´ì „ í„´ì—ì„œ ë…¼ì˜í•œ ì í™”ì‹ êµ¬ì¡°ë¥¼ ë°˜ì˜
# dp[current][visited] = min(cost(current, next) + dp[next][visited | (1<<next)])
# ... (ì½”ë“œ ë‚´ìš©)
```

[Note]
- ì´ì „ ëŒ€í™”ì—ì„œ ë…¼ì˜í•œ ì í™”ì‹ êµ¬ì¡°ë¥¼ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤.
- ìš”ì²­í•˜ì‹  ì œì•½ ì¡°ê±´(ì‹œê°„ ë³µì¡ë„ O(N^2 * 2^N), sys.stdin.readline ì‚¬ìš© ë“±)ì„ ì¤€ìˆ˜í–ˆìŠµë‹ˆë‹¤.
```
```

# ğŸš« ì ˆëŒ€ ê¸ˆì§€
- ë¬¸ì œì˜ ì™„ì „í•œ ì •ë‹µ ì½”ë“œ ì œê³µ (ì²˜ìŒë¶€í„° ëê¹Œì§€ ì™„ì„±ëœ ì½”ë“œ, ë§¥ë½ ì—†ì´ ìš”ì²­ëœ ê²½ìš°)
- ë¬¸ì œ íŠ¹ì • í•µì‹¬ ë¡œì§ì˜ ì™„ì „í•œ êµ¬í˜„ ì œê³µ (ë§¥ë½ ì—†ì´ ìš”ì²­ëœ ê²½ìš°)

# âœ… í—ˆìš© (ë§¥ë½ ê¸°ë°˜)
- **íŒíŠ¸ ìš”ì²­ ì‹œ**: êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ íŒíŠ¸ ì œê³µ (íšŒí”¼ì ì´ì§€ ì•Šê²Œ)
  - ì˜ˆ: "ì í™”ì‹ ìˆ˜ë¦½ì„ ìœ„í•œ íŒíŠ¸" â†’ ì í™”ì‹ì˜ êµ¬ì¡°, ì ‘ê·¼ ë°©ì‹, ì˜ˆì‹œë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´
  - ì˜ˆ: "ë¹„íŠ¸ë§ˆìŠ¤í‚¹ ì‚¬ìš©ë²•" â†’ êµ¬ì²´ì ì¸ ì‚¬ìš© ì˜ˆì‹œì™€ íŒ¨í„´ ì œê³µ
- **ì½”ë“œ ìƒì„± ìš”ì²­ ì‹œ**: ì´ì „ ëŒ€í™” ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ì½”ë“œ ìƒì„±
  - ì´ì „ í„´ì—ì„œ íŒíŠ¸, ì í™”ì‹, ì ‘ê·¼ ë°©ì‹ì´ ë…¼ì˜ëœ ê²½ìš° â†’ ê·¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì½”ë“œ ìƒì„± í—ˆìš©
  - ì˜ˆ: "ì œì•ˆí•´ì£¼ì‹  ì í™”ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”" â†’ ì½”ë“œ ìƒì„± í—ˆìš©
  - ì˜ˆ: "ì´ì „ì— ë§í•œ ë°©ë²•ìœ¼ë¡œ ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”" â†’ ì½”ë“œ ìƒì„± í—ˆìš©
  - ë‹¨, ì²˜ìŒë¶€í„° ì™„ì „í•œ ì •ë‹µ ì½”ë“œë¥¼ ìš”ì²­í•˜ëŠ” ê²½ìš°ëŠ” ì œì™¸

# ğŸ“ ì½”ë“œ ìƒì„± ì‹œ ì£¼ì˜ì‚¬í•­
- ì´ì „ ëŒ€í™” ë§¥ë½ì„ ëª…í™•íˆ ì°¸ì¡°í•˜ì—¬ ì¼ê´€ì„± ìˆëŠ” ì½”ë“œ ìƒì„±
- ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì œì•½ ì¡°ê±´(ì‹œê°„ ë³µì¡ë„, ì…ë ¥ í˜•ì‹ ë“±)ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜
- ì½”ë“œì— ì£¼ì„ì„ ì¶”ê°€í•˜ì—¬ ì´í•´ë¥¼ ë•ê¸°

# Output Formats (Strictly Adhere)
ë‹µë³€ì€ ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ ì¤‘ í•˜ë‚˜ ì´ìƒì„ ì‚¬ìš©:
- **[Syntax Example]**: ë¬¸ë²• ì˜ˆì‹œ (ë¬¸ì œì™€ ë¬´ê´€)
- **[Concept]**: ê°œë… ì„¤ëª… ë˜ëŠ” êµ¬ì²´ì  íŒíŠ¸
- **[Roadmap]**: ë‹¨ê³„ë³„ ì ‘ê·¼ë²•
- **[Question]**: ë°˜ë¬¸ìœ¼ë¡œ ìœ ë„
- **[Code]**: ì½”ë“œ ìƒì„± ìš”ì²­ ì‹œ ì½”ë“œ ì œê³µ (ë§¥ë½ ê¸°ë°˜)

# í†¤
ì¹œì ˆí•˜ê³  ê²©ë ¤í•˜ë˜, ì ì ˆí•œ ìˆ˜ì¤€ì˜ ë„ì›€ì„ ì œê³µ
- íŒíŠ¸ ìš”ì²­ ì‹œ: íšŒí”¼ì ì´ì§€ ì•Šê³  êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´
- ì½”ë“œ ìƒì„± ìš”ì²­ ì‹œ: ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ ì½”ë“œ ì œê³µ

{memory_summary}
"""


def prepare_writer_input(state: MainGraphState) -> Dict[str, Any]:
    """Writer Chain ì…ë ¥ ì¤€ë¹„ (Guide Strategy ê¸°ë°˜)"""
    human_message = state.get("human_message", "")
    messages = state.get("messages", [])
    memory_summary = state.get("memory_summary", "")
    is_guardrail_failed = state.get("is_guardrail_failed", False)
    guardrail_message = state.get("guardrail_message", "")
    
    # Guide Strategy ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    guide_strategy = state.get("guide_strategy", "LOGIC_HINT")  # ê¸°ë³¸ê°’
    keywords = state.get("keywords", [])
    problem_context = state.get("problem_context")
    
    # ì½”ë“œ ìƒì„± ìš”ì²­ ê°ì§€ (ë§¥ë½ ê¸°ë°˜)
    is_code_generation_request = False
    if not is_guardrail_failed:
        message_lower = human_message.lower()
        code_generation_keywords = ["ì½”ë“œ ì‘ì„±", "ì½”ë“œ ìƒì„±", "ì½”ë“œë¥¼ ì‘ì„±", "ì½”ë“œë¥¼ ìƒì„±", "ì½”ë“œ ì‘ì„±í•´", "ì½”ë“œ ìƒì„±í•´"]
        
        # ì½”ë“œ ìƒì„± ìš”ì²­ í‚¤ì›Œë“œ í™•ì¸
        if any(kw in message_lower for kw in code_generation_keywords):
            # ì´ì „ ëŒ€í™”ì—ì„œ íŒíŠ¸ë‚˜ ì í™”ì‹ì´ ë…¼ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
            has_previous_context = False
            if messages:
                # ìµœê·¼ 3í„´ í™•ì¸
                recent_messages = messages[-6:] if len(messages) > 6 else messages
                for msg in recent_messages:
                    if hasattr(msg, 'content'):
                        content = str(msg.content).lower()
                        # íŒíŠ¸, ì í™”ì‹, ì ‘ê·¼ ë°©ì‹ ë“±ì´ ë…¼ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                        context_keywords = ["íŒíŠ¸", "ì í™”ì‹", "ì ‘ê·¼", "ë°©ë²•", "hint", "recurrence", "approach"]
                        if any(ck in content for ck in context_keywords):
                            has_previous_context = True
                            break
            
            # ì´ì „ ë§¥ë½ì´ ìˆê±°ë‚˜, ëª…ì‹œì ìœ¼ë¡œ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•˜ëŠ” ê²½ìš°
            if has_previous_context or any(ref in message_lower for ref in ["ì œì•ˆí•´ì£¼ì‹ ", "ì´ì „", "ì•ì„œ", "ë§í•œ", "ë°”íƒ•ìœ¼ë¡œ"]):
                is_code_generation_request = True
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
    if is_guardrail_failed:
        system_prompt = GUARDRAIL_SYSTEM_PROMPT_TEMPLATE.format(
            guardrail_message=guardrail_message or "ë¶€ì ì ˆí•œ ìš”ì²­"
        )
    else:
        memory_text = f"\n\nì´ì „ ëŒ€í™” ìš”ì•½:\n{memory_summary}" if memory_summary else ""
        keywords_text = ", ".join(keywords) if keywords else "ì—†ìŒ"
        
        # ì½”ë“œ ìƒì„± ìš”ì²­ì¸ ê²½ìš° Guide Strategyë¥¼ GENERATIONìœ¼ë¡œ ë³€ê²½
        if is_code_generation_request:
            guide_strategy = "GENERATION"
        
        system_prompt = create_normal_system_prompt(
            status="SAFE",
            guide_strategy=guide_strategy or "LOGIC_HINT",
            keywords=keywords_text,
            memory_summary=memory_text,
            problem_context=problem_context,
            is_code_generation_request=is_code_generation_request
        )
    
    # ìµœê·¼ ë©”ì‹œì§€ ë³€í™˜ (ìµœëŒ€ 10ê°œ)
    recent_messages = messages[-10:] if len(messages) > 10 else messages
    formatted_messages = []
    for msg in recent_messages:
        if hasattr(msg, 'content'):
            role = getattr(msg, 'type', 'user')
            if role == 'human':
                role = 'user'
            elif role == 'ai':
                role = 'assistant'
            formatted_messages.append({"role": role, "content": msg.content})
    
    return {
        "system_prompt": system_prompt,
        "messages": formatted_messages,
        "human_message": human_message,
        "state": state,  # í›„ì²˜ë¦¬ë¥¼ ìœ„í•´ state ì „ë‹¬
    }


def format_writer_messages(inputs: Dict[str, Any]) -> list:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ LangChain BaseMessage ê°ì²´ë¡œ ë³€í™˜"""
    chat_messages = []
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
    if inputs.get("system_prompt"):
        chat_messages.append(SystemMessage(content=inputs["system_prompt"]))
    
    # ì´ì „ ëŒ€í™” ë©”ì‹œì§€ ë³€í™˜
    for msg in inputs.get("messages", []):
        if isinstance(msg, dict):
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role == "system":
                chat_messages.append(SystemMessage(content=content))
            elif role == "assistant" or role == "ai":
                chat_messages.append(AIMessage(content=content))
            else:
                chat_messages.append(HumanMessage(content=content))
        elif hasattr(msg, 'content'):
            # ì´ë¯¸ BaseMessage ê°ì²´ì¸ ê²½ìš°
            chat_messages.append(msg)
    
    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    if inputs.get("human_message"):
        chat_messages.append(HumanMessage(content=inputs["human_message"]))
    
    return chat_messages


def extract_content(response: Any) -> Dict[str, Any]:
    """LLM ì‘ë‹µì—ì„œ ë‚´ìš© ì¶”ì¶œ"""
    ai_content = response.content if hasattr(response, 'content') else str(response)
    return {
        "ai_content": ai_content,
        "state": response.state if hasattr(response, 'state') else None,
    }


# Writer Chain êµ¬ì„± (ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ìºì‹±)
_writer_chain = None
_writer_llm = None

def get_writer_chain():
    """Writer Chain ìƒì„± (ì‹±ê¸€í†¤ íŒ¨í„´) - Middleware ì ìš©"""
    global _writer_chain, _writer_llm
    
    if _writer_chain is None:
        _writer_llm = get_llm()
        
        # ê¸°ë³¸ Chain: ì…ë ¥ ì¤€ë¹„ -> ë©”ì‹œì§€ í¬ë§· -> LLM í˜¸ì¶œ -> ë‚´ìš© ì¶”ì¶œ (í† í° ì¶”ì¶œì„ ìœ„í•´ LLM ì‘ë‹µ ê°ì²´ë„ ì „ë‹¬)
        def extract_content_with_response(response: Any) -> Dict[str, Any]:
            """LLM ì‘ë‹µì—ì„œ ë‚´ìš©ê³¼ ì‘ë‹µ ê°ì²´ë¥¼ í•¨ê»˜ ë°˜í™˜"""
            ai_content = response.content if hasattr(response, 'content') else str(response)
            return {
                "ai_content": ai_content,
                "_llm_response": response  # í† í° ì¶”ì¶œìš© - LLM ì‘ë‹µ ê°ì²´ ê·¸ëŒ€ë¡œ ì „ë‹¬
            }
        
        _base_writer_chain = (
            RunnableLambda(prepare_writer_input)
            | RunnableLambda(format_writer_messages)
            | _writer_llm  # LLM í˜¸ì¶œ - AIMessage ê°ì²´ ë°˜í™˜
            | RunnableLambda(extract_content_with_response)  # ë‚´ìš© ì¶”ì¶œ ë° ì‘ë‹µ ê°ì²´ ë³´ì¡´
        )
        
        # Middleware ì ìš© (Factory í•¨ìˆ˜ ì‚¬ìš©)
        _writer_chain = wrap_chain_with_middleware(
            _base_writer_chain,
            name="Writer LLM"
        )
    
    return _writer_chain


async def writer_llm(state: MainGraphState) -> Dict[str, Any]:
    """
    AI ë‹µë³€ ìƒì„± (Runnable & Chain êµ¬ì¡°)
    
    ì—­í• :
    - ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ ì½”ë“œ ì‘ì„±
    - íŒíŠ¸ ì œê³µ
    - ë””ë²„ê¹… ë„ì›€
    - ì„¤ëª… ì œê³µ
    """
    import logging
    logger = logging.getLogger(__name__)
    
    human_message = state.get("human_message", "")
    is_guardrail_failed = state.get("is_guardrail_failed", False)
    
    logger.info(f"[Writer LLM] ë‹µë³€ ìƒì„± ì‹œì‘ - message: {human_message[:100]}..., guardrail_failed: {is_guardrail_failed}")
    
    try:
        # Writer Chain ì‹¤í–‰ (ìºì‹±ëœ Chain ì‚¬ìš©)
        chain = get_writer_chain()
        chain_result = await chain.ainvoke(state)
        
        # Chain ê²°ê³¼ì—ì„œ ë‚´ìš©ê³¼ LLM ì‘ë‹µ ê°ì²´ ë¶„ë¦¬
        ai_content = chain_result.get("ai_content", "") if isinstance(chain_result, dict) else str(chain_result)
        llm_response = chain_result.get("_llm_response") if isinstance(chain_result, dict) else None
        
        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ ë° Stateì— ëˆ„ì 
        if llm_response:
            tokens = extract_token_usage(llm_response)
            if tokens:
                accumulate_tokens(state, tokens, token_type="chat")
                logger.debug(f"[Writer LLM] í† í° ì‚¬ìš©ëŸ‰ - prompt: {tokens.get('prompt_tokens')}, completion: {tokens.get('completion_tokens')}, total: {tokens.get('total_tokens')}")
        
        logger.info(f"[Writer LLM] ë‹µë³€ ìƒì„± ì„±ê³µ - ê¸¸ì´: {len(ai_content)} ë¬¸ì")
        
        # í˜„ì¬ í„´ ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸°
        current_turn = state.get("current_turn", 0)
        session_id = state.get("session_id", "unknown")
        
        # ê¸°ì¡´ messages ë°°ì—´ ê¸¸ì´ í™•ì¸ (ìƒˆ ë©”ì‹œì§€ ì¸ë±ìŠ¤ ê³„ì‚°ìš©)
        existing_messages = state.get("messages", [])
        start_msg_idx = len(existing_messages)
        end_msg_idx = start_msg_idx + 1
        
        # Redisì— í„´-ë©”ì‹œì§€ ë§¤í•‘ ì €ì¥
        try:
            from app.infrastructure.cache.redis_client import redis_client
            import asyncio
            
            # ë¹„ë™ê¸°ë¡œ í„´ ë§¤í•‘ ì €ì¥ (ì‹¤íŒ¨í•´ë„ ë©”ì¸ í”Œë¡œìš° ì¤‘ë‹¨ ì•ˆ í•¨)
            asyncio.create_task(
                redis_client.save_turn_mapping(
                    session_id=session_id,
                    turn=current_turn,
                    start_msg_idx=start_msg_idx,
                    end_msg_idx=end_msg_idx
                )
            )
            logger.info(f"[Writer LLM] í„´ ë§¤í•‘ ì €ì¥ ì‹œì‘ - turn: {current_turn}, indices: [{start_msg_idx}, {end_msg_idx}]")
        except Exception as e:
            logger.warning(f"[Writer LLM] í„´ ë§¤í•‘ ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œ): {str(e)}")
        
        # messages ë°°ì—´ì— turn ì •ë³´ í¬í•¨ (4ë²ˆ ë…¸ë“œ í‰ê°€ë¥¼ ìœ„í•´)
        # LangChain BaseMessage ê°ì²´ë¥¼ ì§ì ‘ ìƒì„±í•˜ì—¬ turn ì†ì„± ë³´ì¡´
        from langchain_core.messages import HumanMessage, AIMessage
        
        human_msg = HumanMessage(content=human_message)
        human_msg.turn = current_turn  # turn ì†ì„± ì¶”ê°€
        human_msg.role = "user"  # role ì†ì„± ì¶”ê°€
        human_msg.timestamp = datetime.utcnow().isoformat()
        
        ai_msg = AIMessage(content=ai_content)
        ai_msg.turn = current_turn  # turn ì†ì„± ì¶”ê°€
        ai_msg.role = "assistant"  # role ì†ì„± ì¶”ê°€
        ai_msg.timestamp = datetime.utcnow().isoformat()
        
        new_messages = [human_msg, ai_msg]
        
        # Stateì— ëˆ„ì ëœ í† í° ì •ë³´ë¥¼ resultì— í¬í•¨ (LangGraph ë³‘í•©ì„ ìœ„í•´)
        result = {
            "ai_message": ai_content,
            "messages": new_messages,
            "writer_status": WriterResponseStatus.SUCCESS.value,
            "writer_error": None,
            "updated_at": datetime.utcnow().isoformat(),
        }
        
        # Stateì— ëˆ„ì ëœ í† í° ì •ë³´ í¬í•¨
        if "chat_tokens" in state:
            result["chat_tokens"] = state["chat_tokens"]
        
        return result
        
    except Exception as e:
        logger.error(f"[Writer LLM] ì—ëŸ¬ ë°œìƒ: {str(e)}", exc_info=True)
        error_msg = str(e).lower()
        
        # ì—ëŸ¬ ìœ í˜• ë¶„ë¥˜
        if "rate" in error_msg or "quota" in error_msg:
            status = WriterResponseStatus.FAILED_RATE_LIMIT.value
            logger.warning(f"[Writer LLM] Rate limit ì´ˆê³¼")
        elif "context" in error_msg or "token" in error_msg:
            status = WriterResponseStatus.FAILED_THRESHOLD.value
            logger.warning(f"[Writer LLM] í† í° ì„ê³„ê°’ ì´ˆê³¼")
        else:
            status = WriterResponseStatus.FAILED_TECHNICAL.value
            logger.error(f"[Writer LLM] ê¸°ìˆ ì  ì˜¤ë¥˜: {str(e)}")
        
        return {
            "ai_message": None,
            "writer_status": status,
            "writer_error": str(e),
            "error_message": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "updated_at": datetime.utcnow().isoformat(),
        }


